{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","toc_visible":true,"authorship_tag":"ABX9TyNU7T77byhVjNgpokHoPWdX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#**CHAPTER 7. WRITING A PITCHBOOK**\n","---"],"metadata":{"id":"rgzq7ad4oQsb"}},{"cell_type":"markdown","source":["##REFERENCE"],"metadata":{"id":"nBRtjho5h2Cg"}},{"cell_type":"markdown","source":["https://chatgpt.com/share/6996fb89-c02c-8012-be84-52105d31ac7b"],"metadata":{"id":"SCQp1W1bWDl0"}},{"cell_type":"markdown","source":["##0.CONTEXT"],"metadata":{"id":"qWHgDkx8h5mE"}},{"cell_type":"markdown","source":["**Introduction: How This Notebook Shows AI Drafting an Investment Banking Pitchbook (In Committee-Ready, Simple Terms)**\n","\n","When senior bankers say “we need a pitchbook,” what they mean is not “we need a PowerPoint.” They mean: we need a disciplined, reviewable sequence of thinking that turns a mandate into a coherent narrative, supported by data, structured in standard sections, and packaged so that the decision-makers can move from **context → thesis → valuation logic → risks → next steps** without confusion. In practice, a pitchbook is the visible output of a workflow that is much bigger than the slides themselves: it is a production line of research, drafting, consistency checks, and internal review. Analysts gather data, associates organize story and sections, VPs pressure-test logic and positioning, and MDs refine tone and priorities for the client.\n","\n","This notebook demonstrates how AI can participate in that workflow in a way that is **modular**, **auditable**, and **controlled**. It is not a “chatbot writing a deck.” It is an explicit, state-driven system that drafts pitchbook sections using a hub-and-spoke architecture. The core message for a committee is simple: **AI can generate a pitchbook draft reliably when we treat it like a structured production system with gates, bounded loops, and exported artifacts—rather than as an ungoverned conversational tool.**\n","\n","What follows is a plain-language explanation of the notebook’s logic, what it does, what it does not do, and why this architectural approach is appropriate for a professional banking environment.\n","\n","---\n","\n","**1) The practical problem this solves: speed without losing control**\n","\n","Pitchbook drafting is repetitive but high-stakes. Even when the underlying facts do not change, the production process must:\n","\n","- Produce standard sections quickly (Executive Summary, Company Overview, Market, Comps, Valuation, Risks, Deal Structure).\n","- Maintain consistent terminology and consistent numbers across sections.\n","- Explicitly highlight what is missing, uncertain, or unverified.\n","- Allow senior review to focus on judgment and client positioning, not mechanical formatting.\n","\n","The friction is familiar: analysts assemble data and write first drafts under time pressure; then the team spends hours reconciling inconsistencies, cleaning language, and mapping missing information into follow-up questions. The goal is not to automate investment banking. The goal is to **compress the mechanical drafting time** while increasing transparency around assumptions and gaps.\n","\n","This notebook uses AI in exactly that “drafting accelerator” role. It produces a structured draft for each section, tags gaps, and proposes next questions. The committee should think of this notebook as **a controlled drafting line** for pitchbook content, not a replacement for bankers’ judgment.\n","\n","---\n","\n","**2) What “AI generates a pitchbook” actually means in this notebook**\n","\n","In this notebook, “generate a pitchbook” means:\n","\n","- We provide a structured “input pack” (a bundle of known facts and figures).\n","- We define the list of sections to produce (our pitchbook blueprint).\n","- We run a state machine that assigns each section to a drafting agent.\n","- Each agent writes one section using the same rules: do not invent facts, use only the input pack, list gaps.\n","- We collect all sections and assemble them into a single pitchbook draft document.\n","- We export artifacts for audit and reproducibility: run manifest, graph specification, and final state.\n","\n","This is crucial: the AI is not deciding what the truth is. The AI is drafting within boundaries. The workflow is organized so that **all decisions are visible** and the system can be inspected after the run. That is the difference between “AI as a tool” and “AI as a risk.”\n","\n","---\n","\n","**3) Why the architecture matters: from chat to controlled production**\n","\n","A casual approach to AI would be: open a chat window, ask for a pitchbook, copy/paste results into slides. That is fast, but operationally unsafe. It leads to three predictable problems in a professional setting:\n","\n","- **Invention risk (hallucination):** the model may fabricate numbers, deal comps, or claims.\n","- **Inconsistency risk:** the Executive Summary may contradict the Financial Summary; valuation multiples may drift.\n","- **Reviewability risk:** it is hard to prove what the model was given, what it produced, and why.\n","\n","This notebook solves those problems by design. It uses LangGraph (a graph-based orchestration framework) to implement a deterministic topology. The system is built as a graph with named nodes, controlled routing, bounded loops, and explicit termination. That gives us something we can explain to a committee in simple terms:\n","\n","- **We have a router (the hub) that assigns work.**\n","- **We have section writers (the spokes) that draft one section each.**\n","- **We have an aggregator that assembles everything.**\n","- **We have strict rules: do not fabricate, list gaps, propose follow-ups.**\n","- **We log the run so the output is inspectable and reproducible.**\n","\n","That is the heart of this notebook: architecture first.\n","\n","---\n","\n","**4) The hub-and-spoke constellation: how the pitchbook is “produced”**\n","\n","Investment banking pitchbooks naturally decompose into sections. Analysts already work that way: one person may draft comps, another refines the thesis, another pulls risks and mitigants. We mirror that reality.\n","\n","In the notebook’s graph:\n","\n","- The **HUB_ROUTER** node is the coordinator.\n","- The **WRITE_* nodes** are spokes: each writes exactly one pitchbook section.\n","- The **AGGREGATE** node combines the outputs into one consolidated pitchbook.\n","\n","The system begins at the hub. The hub looks at the state and selects the next section to draft. It then routes to the appropriate spoke. That spoke drafts the section, updates the state with the section output and its gaps, and routes back to the hub. The hub repeats until all sections are done (or until a hard stop condition is reached), then routes to the aggregator, then ends.\n","\n","This is not theoretical. The visualization you saw (the Mermaid diagram) is the operational topology. It is a learning artifact and a governance artifact. It tells you exactly what can happen in a run.\n","\n","---\n","\n","**5) State-driven routing: the workflow is controlled by explicit variables, not “vibes”**\n","\n","In a bank, the problem with many AI demonstrations is that they behave like “magic.” You ask something, you get something, and nobody can explain the intermediate logic. That is not acceptable for professional work.\n","\n","This notebook is designed so the routing is driven by a TypedDict state. That state includes:\n","\n","- What sections remain to be produced (**pending_sections**).\n","- Which section is currently being produced (**current_section**).\n","- The produced outputs (**section_outputs**).\n","- The identified gaps (**section_gaps**).\n","- Run controls (**max_steps**, **steps**).\n","- Termination reasons (**termination_reason**).\n","\n","The important point is that the system does not “choose what to do next” by free-form text heuristics. It chooses by inspecting state. That is why this notebook is teachable and auditable.\n","\n","If you want to explain this to a committee in one sentence: **we run a controlled assembly line where the next task is selected from a queue, not invented by the model.**\n","\n","---\n","\n","**6) The input pack: the model is constrained to a known data bundle**\n","\n","A pitchbook is only as good as the inputs. In real life, those inputs come from research platforms, filings, internal models, and banker judgment. In this notebook, we use synthetic demo data to keep the system reproducible and safe for classroom environments. The key is not that the data is real. The key is that the system forces a discipline:\n","\n","- The AI is given a structured JSON pack.\n","- The AI is instructed to use only that pack.\n","- Any missing or unverifiable item must be listed under **GAPS**.\n","\n","This is an operational control. Instead of pretending the model knows everything, we force it to behave like an analyst who is not allowed to guess. The output is therefore useful as a draft that surfaces what we still need to confirm.\n","\n","This is exactly what you want when presenting to a committee: AI is valuable not because it is “smart,” but because it can draft fast while explicitly marking what it does not know.\n","\n","---\n","\n","**7) The drafting agents: specialized spokes writing specialized sections**\n","\n","Each spoke is a section writer. Practically, you can map them to how a real team works:\n","\n","- **EXEC_SUMMARY** is the associate/VP-level “story spine.”\n","- **COMPANY_OVERVIEW** resembles the analyst pulling company narrative and positioning.\n","- **MARKET_OVERVIEW** resembles the analyst summarizing TAM, growth, tailwinds/headwinds.\n","- **INVESTMENT_THESIS** resembles the internal memo logic: why this asset now, why a buyer cares.\n","- **TRADING_COMPS** and **TRANSACTION_COMPS** resemble comps workstreams.\n","- **FINANCIAL_SUMMARY** resembles a simplified operating model output.\n","- **VALUATION** resembles the synthesis: what ranges and multiples imply, without pretending to have a full model.\n","- **RISKS_AND_MITIGANTS** resembles the diligence-minded voice.\n","- **DEAL_STRUCTURE** resembles the path to transaction and what decisions are needed.\n","\n","All spokes share a common system rule set. That ensures consistency. And because each spoke is a discrete node, we can later swap out, upgrade, or add spokes without breaking the entire system. That is what “modular” means in practice.\n","\n","---\n","\n","**8) Bounded loops: the system can refine once, but it cannot run forever**\n","\n","Professional systems need bounded behavior. You do not want infinite refinement loops. You want fast and predictable execution.\n","\n","This notebook includes a bounded refinement loop per section: if the agent produces content with gaps, it may do exactly one additional “tighten” pass. That is a very practical control: it improves clarity without turning drafting into an endless debate. The loop is explicitly bounded and the bound is configured.\n","\n","Separately, the entire workflow is bounded by a **max_steps** limit. If the system hits the limit, it terminates with a clear reason: **MAX_STEPS_REACHED**. Again, this is a professional-grade control. In a committee setting, you want to say: “We hard-limit iterations; we do not allow runaway autonomy.”\n","\n","---\n","\n","**9) What the notebook does not do (and why that is a feature, not a weakness)**\n","\n","This notebook does not:\n","\n","- Pull live market data.\n","- Validate comps from external sources.\n","- Build a full valuation model.\n","- Make investment recommendations.\n","- Replace human judgment on positioning, disclosures, or client strategy.\n","\n","Those are not omissions. They are boundaries. The notebook is an architectural demonstration: how to structure AI work in a bank in a way that is safe and reviewable. If we later connect retrieval tools, data rooms, or valuation models, the topology already supports it. But we start with the governance skeleton first.\n","\n","In committee terms: **this is the controlled scaffolding on which more functionality can be safely added.**\n","\n","---\n","\n","**10) Why LangGraph is used: we want explicit topology, not hidden control flow**\n","\n","LangGraph is used because it forces us to formalize the workflow as a graph with:\n","\n","- Named nodes (who does what).\n","- Conditional edges (when we go where).\n","- Explicit end state (how we stop).\n","- State schema (what variables exist).\n","- Deterministic transitions (what updates happen).\n","\n","This is fundamentally different from a script that calls an LLM several times. A script can work, but it is harder to reason about, harder to visualize, and harder to audit. The graph is the governance object.\n","\n","In the notebook, the Mermaid diagram is not decoration. It is how we teach the topology and how we demonstrate control.\n","\n","---\n","\n","**11) The artifacts: auditability is not a slogan, it is exported files**\n","\n","At the end of the run, the notebook exports:\n","\n","- **run_manifest.json**: what model was used, what config was used, environment versions, timestamps, controls.\n","- **graph_spec.json**: a structural specification of the topology (nodes, edges, entry, end, loop bounds).\n","- **final_state.json**: the full state after execution, including outputs and gaps.\n","\n","These artifacts matter because they allow a team to answer questions like:\n","\n","- “What exactly did we run?”\n","- “With which model and parameters?”\n","- “What did the system know at the start?”\n","- “What did it produce in each section?”\n","- “What gaps did it identify?”\n","- “Why did it stop?”\n","\n","This is what turns AI from a demo into something that can be governed.\n","\n","---\n","\n","**12) How you should position this to your bosses**\n","\n","If you are presenting this notebook to a committee, the best framing is:\n","\n","- We are not automating banking.\n","- We are building an **AI drafting line** with explicit controls.\n","- The system is modular: we can replace spokes, add retrieval, add gates.\n","- The system is auditable: we export artifacts, visualize topology, and bound loops.\n","- The output is a **first draft** and a **gap map**, accelerating analyst work and improving review focus.\n","\n","The committee will immediately understand that this is similar to what happens today, but faster and more structured. You can also emphasize that the system is designed so that errors are detectable: gaps are listed, and outputs can be reviewed section by section.\n","\n","---\n","\n","**13) What success looks like in a banking workflow**\n","\n","In real deployment terms, success would look like:\n","\n","- Analyst provides a curated input pack: comps, transactions, financial snapshot, market bullets.\n","- The system drafts a pitchbook skeleton and first text.\n","- The system produces a consolidated gap list and follow-up questions.\n","- The deal team reviews, edits, and replaces placeholders with validated facts.\n","- The system can be rerun as inputs improve, producing a clean updated draft.\n","\n","This matches real life: pitchbooks are iterative. The difference is that iteration becomes faster and more transparent.\n","\n","---\n","\n","**14) The key takeaway**\n","\n","This notebook demonstrates a disciplined claim:\n","\n","**AI can generate pitchbook drafts reliably when we structure the work as a state-driven hub-and-spoke system with explicit routing, bounded refinement, and exported audit artifacts.**\n","\n","That is the point you want your bosses to leave with. The value is not just speed. The value is that the workflow becomes **more standardized, more reviewable, and more governable** than the traditional “copy and paste from a chat window” approach.\n","\n","If the committee wants a simple mental model, give them this:\n","\n","- The hub is the project manager.\n","- The spokes are analysts assigned to sections.\n","- The aggregator is the associate assembling the book.\n","- The state is the shared deal folder.\n","- The artifacts are the audit trail.\n","\n","And, importantly, the AI is constrained: it drafts only from what we provide, and it must explicitly admit what is missing. That is what makes it safe enough to be a professional tool rather than a novelty.\n","\n","---\n","\n","**What this introduction is preparing you to show next**\n","\n","After this framing, the committee will be ready to see:\n","\n","- The Mermaid diagram (topology as governance).\n","- The input pack (what the system knows).\n","- A sample run output (what it produces).\n","- The exported artifacts (how we control and review it).\n","\n","At that point the conversation can shift from “Is AI safe?” to “How do we integrate this safely into our drafting workflow?” which is exactly where you want it.\n"],"metadata":{"id":"pF_yDrZ7h6_y"}},{"cell_type":"markdown","source":["##1.LIBRARIES AND ENVIRONMENT"],"metadata":{"id":"yeIrVzqFh7fZ"}},{"cell_type":"markdown","source":["**Cell 1 — Install and runtime hardening (why we start here)**\n","\n","This first cell is the “foundation slab” of the notebook. In an institutional environment, the most common reason a supposedly correct AI workflow fails is not logic—it is environment drift. Google Colab comes with many preinstalled packages, and some of them can conflict with the specific versions we need for LangGraph and the Anthropic client. If we do not control that, two people can run the same notebook and get different behavior. For a committee, the message is simple: **reproducibility starts with deterministic dependencies**.\n","\n","The cell does three key things. First, it removes a known conflict: `langgraph-prebuilt`. Colab sometimes pulls in “helper” packages that shadow or interfere with the core `langgraph` APIs. We explicitly uninstall it, but we do so in a non-fatal way (`|| true`) so the cell remains robust even if the package was not installed. Second, we install a pinned stack using `--force-reinstall`. That flag matters: it overwrites preinstalled versions and prevents subtle incompatibilities. The pins (requests, numpy, pydantic, httpx, httpcore, langgraph, langchain, langchain-core) are chosen to align with Colab constraints and to keep the graph and LLM integration stable. Third, we run `pip check`. This is a quick dependency integrity test. We do not want hidden conflicts; we want them visible.\n","\n","After installation, the cell sets the deterministic tone: we import the standard libraries we will use (including typing types for the explicit state schema), seed randomness, and set `PYTHONHASHSEED` to stabilize hash behavior. We then print a structured CONFIG and VERSIONS block. This is not cosmetic. In professional workflows, you always want “what did we run, and with what versions?” printed immediately so that debugging and governance do not rely on memory.\n","\n","The final part checks expected versions against actual versions and prints mismatches. That is a quality gate: if versions drift, you see it before you trust outputs. In short, Cell 1 is how we turn a notebook from a demo into a controlled artifact: **pinned dependencies, deterministic settings, and a visible environment fingerprint**.\n"],"metadata":{"id":"F4hFUJN0xPB0"}},{"cell_type":"code","source":["# CELL 1/10 — Install + core imports (Colab hardened + remove langgraph-prebuilt conflicts) — N7 aligned\n","\n","# 1) Remove the conflicting prebuilt package if present (non-fatal if absent)\n","!pip -q uninstall -y langgraph-prebuilt || true\n","\n","# 2) Install pinned stack aligned with Colab constraints (force reinstall to defeat preinstalled drift)\n","!pip -q install --upgrade --force-reinstall \\\n","  \"requests==2.32.4\" \\\n","  \"numpy==2.0.2\" \\\n","  \"pydantic==2.12.3\" \\\n","  \"httpx==0.28.1\" \"httpcore==1.0.5\" \\\n","  \"langgraph==0.2.39\" \"langchain==0.3.14\" \"langchain-core==0.3.40\" \\\n","  \"anthropic>=0.34.0\"\n","\n","# sanity: dependency integrity (review if it complains)\n","!pip -q check || true\n","\n","import os, json, uuid, time, random, hashlib, platform, re, textwrap\n","import datetime as _dt\n","from typing import TypedDict, Literal, Dict, Any, List, Optional, Callable, Tuple\n","from typing_extensions import Annotated\n","import operator\n","\n","import httpx\n","from google.colab import userdata\n","from IPython.display import HTML, display\n","from langgraph.graph import StateGraph, END\n","\n","random.seed(7)\n","os.environ[\"PYTHONHASHSEED\"] = \"7\"\n","\n","import importlib.metadata as md\n","def _ver(pkg: str) -> str:\n","    try:\n","        return md.version(pkg)\n","    except Exception:\n","        return \"missing\"\n","\n","def utc_now_iso() -> str:\n","    return _dt.datetime.now(_dt.timezone.utc).isoformat()\n","\n","CONFIG: Dict[str, Any] = {\n","    \"project\": \"AA-FIN-LG-2026\",\n","    \"notebook\": \"N7 — IB Pitchbook: Hub-and-Spoke Constellation\",\n","    \"model\": \"claude-haiku-4-5-20251001\",\n","    \"temperature\": 0.0,\n","    \"max_tokens\": 900,\n","    \"seed\": 7,\n","    \"mermaid_version\": \"10.6.1\",\n","    \"bounded_retries_per_section\": 1,\n","}\n","\n","VERSIONS = {\n","    \"python\": platform.python_version(),\n","    \"platform\": platform.platform(),\n","    \"requests\": _ver(\"requests\"),\n","    \"numpy\": _ver(\"numpy\"),\n","    \"pydantic\": _ver(\"pydantic\"),\n","    \"httpx\": _ver(\"httpx\"),\n","    \"httpcore\": _ver(\"httpcore\"),\n","    \"langgraph\": _ver(\"langgraph\"),\n","    \"langchain\": _ver(\"langchain\"),\n","    \"langchain-core\": _ver(\"langchain-core\"),\n","    \"anthropic\": _ver(\"anthropic\"),\n","    \"langgraph-prebuilt\": _ver(\"langgraph-prebuilt\"),  # should become \"missing\"\n","}\n","\n","print(\"CONFIG:\", json.dumps(CONFIG, indent=2))\n","print(\"VERSIONS:\", json.dumps(VERSIONS, indent=2))\n","print(\"UTC_NOW:\", utc_now_iso())\n","\n","EXPECTED = {\n","    \"requests\": \"2.32.4\",\n","    \"numpy\": \"2.0.2\",\n","    \"pydantic\": \"2.12.3\",\n","    \"httpx\": \"0.28.1\",\n","    \"httpcore\": \"1.0.5\",\n","    \"langgraph\": \"0.2.39\",\n","    \"langchain\": \"0.3.14\",\n","    \"langchain-core\": \"0.3.40\",\n","}\n","mismatch = {k: {\"expected\": EXPECTED[k], \"got\": VERSIONS.get(k)} for k in EXPECTED if VERSIONS.get(k) != EXPECTED[k]}\n","print(\"VERSION_MISMATCH:\", json.dumps(mismatch, indent=2))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vr58yP-dr8oG","executionInfo":{"status":"ok","timestamp":1771501261769,"user_tz":360,"elapsed":29153,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"c5714f36-0e2b-4da0-ab2a-281a805d09b7"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping langgraph-prebuilt as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCONFIG: {\n","  \"project\": \"AA-FIN-LG-2026\",\n","  \"notebook\": \"N7 \\u2014 IB Pitchbook: Hub-and-Spoke Constellation\",\n","  \"model\": \"claude-haiku-4-5-20251001\",\n","  \"temperature\": 0.0,\n","  \"max_tokens\": 900,\n","  \"seed\": 7,\n","  \"mermaid_version\": \"10.6.1\",\n","  \"bounded_retries_per_section\": 1\n","}\n","VERSIONS: {\n","  \"python\": \"3.12.12\",\n","  \"platform\": \"Linux-6.6.105+-x86_64-with-glibc2.35\",\n","  \"requests\": \"2.32.4\",\n","  \"numpy\": \"2.0.2\",\n","  \"pydantic\": \"2.12.3\",\n","  \"httpx\": \"0.28.1\",\n","  \"httpcore\": \"1.0.5\",\n","  \"langgraph\": \"0.2.39\",\n","  \"langchain\": \"0.3.14\",\n","  \"langchain-core\": \"0.3.40\",\n","  \"anthropic\": \"0.82.0\",\n","  \"langgraph-prebuilt\": \"missing\"\n","}\n","UTC_NOW: 2026-02-19T11:41:01.703885+00:00\n","VERSION_MISMATCH: {}\n"]}]},{"cell_type":"markdown","source":["##2.CONFIGURATION"],"metadata":{"id":"aOzs2vNWiEjE"}},{"cell_type":"markdown","source":["###2.1.OVERVIEW"],"metadata":{"id":"_Z7ptw6JiIcM"}},{"cell_type":"markdown","source":["**Cell 2 — Configuration, state schema, and the “input pack contract”**\n","\n","Cell 2 is where we define the “rules of the world” for the workflow. The most important concept for this entire project is that **state drives routing**. To make that real, we declare an explicit TypedDict state schema. This is not just typing hygiene; it is governance. By listing fields up front, we force ourselves to answer: What does the system know? What does it track? What does it decide from? What does it output? A pitchbook is a multi-section product, so the state includes fields that mirror that reality: which sections are pending, which section is currently being drafted, what outputs have been produced, what gaps have been detected, and why the run terminated.\n","\n","In this notebook, the state is called `PitchbookState`. It includes run controls (`run_id`, timestamps, step limits), deal context (mandate type, audience, style, company identifiers), the structured `input_pack`, and coordination fields (`pending_sections`, `current_section`). It also includes outputs (`section_outputs`, `assembled_pitchbook`) and governance markers (`refusal`, `refusal_reason`, `termination_reason`). The key idea: we are not hoping the model “remembers” anything. The graph reads and writes to state, and state is the single source of truth.\n","\n","Cell 2 also defines the list of pitchbook sections. This is the hub-and-spoke blueprint: the hub will assign these sections one by one to specialized writers. The sections are typed (a Literal union) so they are not free-form strings. Again, that is a control: fewer accidental errors, more predictable routing.\n","\n","Finally, Cell 2 constructs a deterministic synthetic input pack. In real banking, this pack would come from filings, databases, and internal models. Here it is synthetic so the notebook remains reproducible and safe for teaching. But the structure is realistic: trading comps with multiples, transaction comps with rationale, a financial snapshot, market attributes, deal options, and a risk list. The pack also includes a governance note: “synthetic demo only” and “Not verified.”\n","\n","This “input pack contract” is essential to the committee story. It shows we can constrain AI: **draft using only known inputs, and surface missing information as gaps**. Cell 2 is where we formalize that discipline in code.\n"],"metadata":{"id":"jAsIm-aiiLkE"}},{"cell_type":"markdown","source":["###2.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"AK_Iz2Q4iL4X"}},{"cell_type":"code","source":["# CELL 2/10 — Configuration, explicit TypedDict state, synthetic pitch inputs (deterministic)\n","\n","PitchSection = Literal[\n","    \"COMPANY_OVERVIEW\",\n","    \"MARKET_OVERVIEW\",\n","    \"INVESTMENT_THESIS\",\n","    \"TRADING_COMPS\",\n","    \"TRANSACTION_COMPS\",\n","    \"FINANCIAL_SUMMARY\",\n","    \"VALUATION\",\n","    \"RISKS_AND_MITIGANTS\",\n","    \"DEAL_STRUCTURE\",\n","    \"EXEC_SUMMARY\",\n","]\n","\n","class PitchbookState(TypedDict, total=False):\n","    # run controls\n","    run_id: str\n","    ts_utc: str\n","    max_steps: int\n","    steps: int\n","\n","    # user request\n","    mandate_type: Literal[\"IPO\", \"M&A_BUYSIDE\", \"M&A_SELLSIDE\", \"DEBT_FINANCING\"]\n","    audience: Literal[\"IC\", \"CFO\", \"BOARD\", \"LENDERS\"]\n","    style: Literal[\"CRISP\", \"INSTITUTIONAL\", \"TEACHING\"]\n","    company_name: str\n","    ticker: str\n","    sector: str\n","    geography: str\n","\n","    # synthetic pack (inputs)\n","    input_pack: Dict[str, Any]\n","\n","    # hub-and-spoke coordination\n","    pending_sections: List[PitchSection]\n","    current_section: Optional[PitchSection]\n","    section_outputs: Dict[str, str]\n","    section_gaps: Dict[str, List[str]]\n","\n","    # assembly\n","    assembled_pitchbook: str\n","\n","    # governance\n","    refusal: bool\n","    refusal_reason: Optional[str]\n","    termination_reason: Optional[str]\n","\n","CONFIG: Dict[str, Any] = {\n","    \"model\": \"claude-haiku-4-5-20251001\",  # strict lock\n","    \"temperature\": 0.0,\n","    \"max_tokens\": 900,\n","    \"mermaid_version\": \"10.6.1\",\n","    \"bounded_retries_per_section\": 1,  # bounded loop dimension (per spoke)\n","}\n","\n","def deterministic_synthetic_input_pack(seed: int = 7) -> Dict[str, Any]:\n","    rnd = random.Random(seed)\n","    # Synthetic comps / transactions / financials (toy but structured)\n","    trading_comps = [\n","        {\"name\": \"ApexSoft\", \"ticker\": \"APXS\", \"ev_rev\": 6.2, \"ev_ebitda\": 22.5, \"growth_yoy\": 0.18, \"gross_margin\": 0.74},\n","        {\"name\": \"NorthCloud\", \"ticker\": \"NCLD\", \"ev_rev\": 5.4, \"ev_ebitda\": 19.8, \"growth_yoy\": 0.15, \"gross_margin\": 0.71},\n","        {\"name\": \"PrismData\", \"ticker\": \"PRSD\", \"ev_rev\": 7.1, \"ev_ebitda\": 25.2, \"growth_yoy\": 0.22, \"gross_margin\": 0.77},\n","    ]\n","    txn_comps = [\n","        {\"acquirer\": \"MegaTech\", \"target\": \"SignalWorks\", \"year\": 2024, \"ev_rev\": 8.0, \"rationale\": \"AI-enabled workflow consolidation\"},\n","        {\"acquirer\": \"OmniSuite\", \"target\": \"LedgerIQ\", \"year\": 2023, \"ev_rev\": 6.7, \"rationale\": \"vertical expansion into finance ops\"},\n","    ]\n","    financials = {\n","        \"currency\": \"USD\",\n","        \"fy2023\": {\"revenue\": 420, \"gross_margin\": 0.72, \"ebitda_margin\": 0.24, \"fcf_margin\": 0.18},\n","        \"fy2024\": {\"revenue\": 495, \"gross_margin\": 0.73, \"ebitda_margin\": 0.25, \"fcf_margin\": 0.19},\n","        \"fy2025e\": {\"revenue\": 585, \"gross_margin\": 0.74, \"ebitda_margin\": 0.26, \"fcf_margin\": 0.20},\n","        \"net_debt\": 120,\n","    }\n","    market = {\n","        \"tam_usd_bn\": 35,\n","        \"cagr\": 0.14,\n","        \"subsegments\": [\"AP automation\", \"Treasury ops\", \"Spend analytics\", \"Invoice fraud controls\"],\n","        \"tailwinds\": [\"regulatory reporting complexity\", \"AI-assisted workflow automation\", \"cloud migration\"],\n","        \"headwinds\": [\"budget scrutiny\", \"long enterprise sales cycles\"],\n","    }\n","    deal = {\n","        \"transaction\": \"M&A_SELLSIDE\",\n","        \"use_of_proceeds\": [\"partial liquidity for sponsors\", \"accelerate R&D\", \"selective tuck-in M&A\"],\n","        \"structure_options\": [\"stock sale\", \"merger\", \"dual-track IPO readiness\"],\n","    }\n","    risks = [\n","        {\"risk\": \"Competitive displacement\", \"mitigant\": \"domain-specific models + sticky integrations\"},\n","        {\"risk\": \"Customer concentration\", \"mitigant\": \"pipeline diversification + multi-year contracts\"},\n","        {\"risk\": \"Macro IT spend slowdown\", \"mitigant\": \"ROI-led positioning + modular pricing\"},\n","    ]\n","    return {\n","        \"trading_comps\": trading_comps,\n","        \"transaction_comps\": txn_comps,\n","        \"financials\": financials,\n","        \"market\": market,\n","        \"deal\": deal,\n","        \"risks\": risks,\n","        \"notes\": {\n","            \"data_provenance\": \"synthetic_demo_only\",\n","            \"verification_status\": \"Not verified\",\n","            \"instruction\": \"Use as illustrative inputs; flag gaps; do not invent facts beyond pack.\"\n","        }\n","    }\n","\n","DEFAULT_SECTIONS: List[PitchSection] = [\n","    \"EXEC_SUMMARY\",\n","    \"COMPANY_OVERVIEW\",\n","    \"MARKET_OVERVIEW\",\n","    \"INVESTMENT_THESIS\",\n","    \"TRADING_COMPS\",\n","    \"TRANSACTION_COMPS\",\n","    \"FINANCIAL_SUMMARY\",\n","    \"VALUATION\",\n","    \"RISKS_AND_MITIGANTS\",\n","    \"DEAL_STRUCTURE\",\n","]\n","\n","print(\"CONFIG:\", CONFIG)\n","print(\"DEFAULT_SECTIONS:\", DEFAULT_SECTIONS)\n","print(\"SYNTHETIC_PACK_KEYS:\", list(deterministic_synthetic_input_pack().keys()))\n"],"metadata":{"id":"IDDyGwjWiO56","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771501261867,"user_tz":360,"elapsed":90,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"0d57c618-8df3-4b06-cb1d-3ee40ba1b194"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["CONFIG: {'model': 'claude-haiku-4-5-20251001', 'temperature': 0.0, 'max_tokens': 900, 'mermaid_version': '10.6.1', 'bounded_retries_per_section': 1}\n","DEFAULT_SECTIONS: ['EXEC_SUMMARY', 'COMPANY_OVERVIEW', 'MARKET_OVERVIEW', 'INVESTMENT_THESIS', 'TRADING_COMPS', 'TRANSACTION_COMPS', 'FINANCIAL_SUMMARY', 'VALUATION', 'RISKS_AND_MITIGANTS', 'DEAL_STRUCTURE']\n","SYNTHETIC_PACK_KEYS: ['trading_comps', 'transaction_comps', 'financials', 'market', 'deal', 'risks', 'notes']\n"]}]},{"cell_type":"markdown","source":["##3.ANTRHOPIC CLIENT"],"metadata":{"id":"zn9_gpBPh_yn"}},{"cell_type":"markdown","source":["###3.1.OVERVIEW"],"metadata":{"id":"6Sg0lGxQiBj6"}},{"cell_type":"markdown","source":["**Cell 3 — Secure model access and the auditable LLM call wrapper**\n","\n","Cell 3 introduces the model in a controlled way. In professional settings, the failure mode is often not “wrong output,” but “unclear provenance”: Who called the model? With what key? With what parameters? What did we send? This cell addresses that by centralizing the Anthropic client creation and the text-call wrapper in a single place.\n","\n","The first function, `get_anthropic_client()`, reads the API key from Colab Secrets using `userdata.get(\"ANTHROPIC_API_KEY\")`. This is an operational control. We do not hardcode secrets, we do not paste them into notebooks, and we do not store them in variables that end up in exported artifacts. If the key is missing, the cell fails with a clear message, which is better than partial silent failure.\n","\n","The second function, `llm_text(...)`, is the minimal, deterministic interface to the model. It takes the model name, system prompt, user prompt, max tokens, and temperature. We explicitly set temperature to 0.0 in CONFIG, because we want stable drafting behavior for teaching and for governance. The wrapper also parses Anthropic message content in a deterministic way: it concatenates only the text blocks. That matters because multi-block responses can exist and we want consistent extraction.\n","\n","The system prompt (`SYSTEM_BASE`) is intentionally restrictive. It is the behavioral contract for every drafting agent in the notebook: use only the input pack; do not invent facts; list gaps; keep structure crisp; mark verification status as not verified. This makes the model a disciplined drafter rather than an imaginative writer.\n","\n","In committee terms, Cell 3 is where we put the model “inside the box.” We do not let it roam; we do not ask it to browse; we do not treat it as a source of truth. We treat it as a drafting engine that operates under a compliance-style instruction set. That also makes it easier to improve later: if we want stronger refusal behavior, tighter formatting, or safer language, we edit one system prompt rather than scattering prompt logic across nodes.\n","\n","So Cell 3 accomplishes three things: secure key handling, deterministic model invocation, and a consistent behavioral policy for all spokes. It is the bridge between our governed orchestration and the external LLM capability.\n"],"metadata":{"id":"Tom4mTlAsUhD"}},{"cell_type":"markdown","source":["###3.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"SqN4lQyuiE7Y"}},{"cell_type":"code","source":["# CELL 3/10 — Anthropic client init (Colab secret ALL CAPS) + minimal, auditable LLM call wrapper\n","\n","from anthropic import Anthropic\n","\n","def get_anthropic_client() -> Anthropic:\n","    key = userdata.get(\"ANTHROPIC_API_KEY\")  # strict: ALL CAPS\n","    if not key or not isinstance(key, str):\n","        raise RuntimeError(\"Missing Colab secret ANTHROPIC_API_KEY (ALL CAPS). Add it in Colab → Secrets.\")\n","    return Anthropic(api_key=key)\n","\n","def llm_text(client: Anthropic, *, model: str, system: str, user: str, max_tokens: int, temperature: float) -> str:\n","    msg = client.messages.create(\n","        model=model,\n","        max_tokens=max_tokens,\n","        temperature=temperature,\n","        system=system,\n","        messages=[{\"role\": \"user\", \"content\": user}],\n","    )\n","    # Anthropic messages API returns content blocks; we take concatenated text blocks deterministically\n","    parts: List[str] = []\n","    for block in msg.content:\n","        if getattr(block, \"type\", None) == \"text\":\n","            parts.append(block.text)\n","    return \"\\n\".join(parts).strip()\n","\n","SYSTEM_BASE = \"\"\"You are an investment-banking analyst drafting pitchbook sections.\n","Non-negotiables:\n","- Use ONLY the provided input_pack; do NOT fabricate facts.\n","- If something is missing, list it under \"GAPS\" and proceed with a conservative placeholder.\n","- Keep output structured, crisp, and board-ready.\n","- Verification status: Not verified (synthetic demo).\n","\"\"\"\n","\n","print(\"Anthropic wrapper ready. (Client will be created at run time.)\")\n"],"metadata":{"id":"rGOCADsbjRic","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771501261893,"user_tz":360,"elapsed":19,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"caf7376c-ec82-4590-dfb8-258077741902"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Anthropic wrapper ready. (Client will be created at run time.)\n"]}]},{"cell_type":"markdown","source":["##4.AGENT NODE"],"metadata":{"id":"7CdFvriyiHh-"}},{"cell_type":"markdown","source":["###4.1.OVERVIEW"],"metadata":{"id":"r7AlxPj_iI11"}},{"cell_type":"markdown","source":["**Cell 4 — AgentNode abstraction and section-writer “spokes” with bounded refinement**\n","\n","Cell 4 is where the notebook becomes an agentic architecture rather than a script. The project requirement is an AgentNode abstraction, and this cell implements it in a clean way: an AgentNode has a name and a function, and calling it takes in state and returns updated state. That is the “unit of work” in our graph. It is modular, testable, and consistent across nodes.\n","\n","Next, the cell defines how each pitchbook section is drafted. The function `_section_prompt(section, state)` builds two prompts: a system prompt (policy) and a user prompt (task specification). The user prompt includes mandate, audience, style, company identifiers, and the entire input pack rendered as JSON. This is important: the model has no hidden memory. Everything it needs is explicitly provided. The prompt also enforces a strict output format: TITLE, CONTENT, GAPS, NEXT. That structure is what makes the output reviewable and easy to assemble.\n","\n","Then we add gap extraction logic with `_extract_gaps(text)`. This is intentionally simple and deterministic: we scan lines after the “GAPS:” header until “NEXT:”. The goal is not perfect parsing; the goal is to convert “missing info” into an explicit list that we can store in state and later consolidate across sections.\n","\n","The most important design element in this cell is the bounded self-check. For each section, if gaps exist, we allow exactly one “tighten” refinement pass (configured by `bounded_retries_per_section`). This is a practical compromise. In real life, analysts iterate: a first draft is messy, then tightened. But we must cap autonomy. So we do one refinement pass for clarity and decision usefulness, explicitly instructing “do not add facts.” This gives better writing without enabling endless loops.\n","\n","Finally, the cell constructs all spokes: `SPOKES = {sec: make_section_writer(sec)}`. Each section becomes a distinct node in the graph with a stable name like `WRITE_EXEC_SUMMARY`. That is the “constellation” pattern: multiple specialized workers, each responsible for a well-scoped output.\n","\n","In committee language, Cell 4 creates your virtual pitchbook team. The team members share a common policy, they work from the same deal folder (state + input pack), and they are allowed one small refinement pass—no more. This matches professional drafting: fast initial output, quick tightening, then human review.\n"],"metadata":{"id":"ueNd8fXviKfV"}},{"cell_type":"markdown","source":["###4.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"WYXG5UW3iK2O"}},{"cell_type":"code","source":["# CELL 4/10 — AgentNode abstraction + spoke factories (section writers with bounded self-check)\n","\n","class AgentNode:\n","    \"\"\"\n","    Required abstraction: deterministic state-in/state-out node wrapper.\n","    \"\"\"\n","    def __init__(self, name: str, fn: Callable[[PitchbookState], PitchbookState]):\n","        self.name = name\n","        self.fn = fn\n","\n","    def __call__(self, state: PitchbookState) -> PitchbookState:\n","        return self.fn(state)\n","\n","def _section_prompt(section: PitchSection, state: PitchbookState) -> Tuple[str, str]:\n","    pack = state[\"input_pack\"]\n","    mandate = state[\"mandate_type\"]\n","    audience = state[\"audience\"]\n","    style = state[\"style\"]\n","    company = f'{state[\"company_name\"]} ({state[\"ticker\"]})'\n","    sector = state[\"sector\"]\n","    geo = state[\"geography\"]\n","\n","    user = f\"\"\"\n","MANDATE: {mandate}\n","AUDIENCE: {audience}\n","STYLE: {style}\n","\n","COMPANY: {company}\n","SECTOR: {sector}\n","GEOGRAPHY: {geo}\n","\n","INPUT_PACK (JSON):\n","{json.dumps(pack, indent=2)}\n","\n","TASK:\n","Draft the pitchbook section: {section}\n","\n","FORMAT (strict):\n","- TITLE: one line\n","- CONTENT: bullet points, concise, professional\n","- GAPS: bullet list of missing info or unverifiable items (if none, write \"None\")\n","- NEXT: 1-3 suggested follow-up questions for bankers\n","\n","CONSTRAINTS:\n","- No invented facts.\n","- Use numbers only if present in input_pack.\n","\"\"\"\n","    # Section-specific guidance (kept deterministic)\n","    system = SYSTEM_BASE + f\"\\nSection focus: {section}.\\n\"\n","    return system, user\n","\n","def _extract_gaps(text: str) -> List[str]:\n","    # Deterministic, simple parse: take lines after \"GAPS:\" until \"NEXT:\" (best-effort)\n","    lines = [ln.strip() for ln in text.splitlines()]\n","    gaps: List[str] = []\n","    in_gaps = False\n","    for ln in lines:\n","        if ln.upper().startswith(\"GAPS:\"):\n","            in_gaps = True\n","            continue\n","        if ln.upper().startswith(\"NEXT:\"):\n","            in_gaps = False\n","        if in_gaps:\n","            if ln and ln != \"None\" and ln != \"- None\":\n","                gaps.append(ln.lstrip(\"-\").strip())\n","    return gaps\n","\n","def make_section_writer(section: PitchSection) -> AgentNode:\n","    def _fn(state: PitchbookState) -> PitchbookState:\n","        if state.get(\"refusal\"):\n","            return state\n","\n","        client = get_anthropic_client()\n","        system, user = _section_prompt(section, state)\n","\n","        draft = llm_text(\n","            client,\n","            model=CONFIG[\"model\"],\n","            system=system,\n","            user=user,\n","            max_tokens=int(CONFIG[\"max_tokens\"]),\n","            temperature=float(CONFIG[\"temperature\"]),\n","        )\n","\n","        gaps = _extract_gaps(draft)\n","\n","        # Bounded self-check loop: if gaps are huge, do exactly one refinement pass to tighten language\n","        # (No new facts; only clarity and explicit gap listing)\n","        retries_left = state.get(\"_retries_left\", {}).get(section, CONFIG[\"bounded_retries_per_section\"])\n","        if gaps and retries_left > 0:\n","            tighten_user = user + \"\\n\\nREFINE:\\nRewrite to be tighter and more decision-useful. Do NOT add facts. Keep gaps explicit.\"\n","            refined = llm_text(\n","                client,\n","                model=CONFIG[\"model\"],\n","                system=system,\n","                user=tighten_user,\n","                max_tokens=int(CONFIG[\"max_tokens\"]),\n","                temperature=float(CONFIG[\"temperature\"]),\n","            )\n","            draft = refined\n","            gaps = _extract_gaps(draft)\n","            # decrement retry\n","            new_retries = dict(state.get(\"_retries_left\", {}))\n","            per = dict(new_retries.get(section, {})) if isinstance(new_retries.get(section, {}), dict) else {}\n","            new_retries[section] = max(0, retries_left - 1)\n","            state[\"_retries_left\"] = new_retries\n","\n","        out = dict(state.get(\"section_outputs\", {}))\n","        out[str(section)] = draft\n","\n","        gap_map = dict(state.get(\"section_gaps\", {}))\n","        gap_map[str(section)] = gaps\n","\n","        state[\"section_outputs\"] = out\n","        state[\"section_gaps\"] = gap_map\n","        state[\"steps\"] = int(state.get(\"steps\", 0)) + 1\n","        return state\n","\n","    return AgentNode(f\"WRITE_{section}\", _fn)\n","\n","SPOKES: Dict[PitchSection, AgentNode] = {sec: make_section_writer(sec) for sec in DEFAULT_SECTIONS}\n","print(\"Spokes built:\", list(SPOKES.keys()))\n"],"metadata":{"id":"zwXf8QYdjSeI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771501261959,"user_tz":360,"elapsed":38,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"275f9dbd-5c48-4b25-ceb5-97325163ae4e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Spokes built: ['EXEC_SUMMARY', 'COMPANY_OVERVIEW', 'MARKET_OVERVIEW', 'INVESTMENT_THESIS', 'TRADING_COMPS', 'TRANSACTION_COMPS', 'FINANCIAL_SUMMARY', 'VALUATION', 'RISKS_AND_MITIGANTS', 'DEAL_STRUCTURE']\n"]}]},{"cell_type":"markdown","source":["##5.HUB ROUTER NODE"],"metadata":{"id":"PJydwmXniOfj"}},{"cell_type":"markdown","source":["###5.1.OVERVIEW"],"metadata":{"id":"MeZ0d501iP49"}},{"cell_type":"markdown","source":["**Cell 5 — The hub router and conditional routing logic (the core of hub-and-spoke)**\n","\n","Cell 5 implements the “project manager” of the system: the hub router. This is the centerpiece of the hub-and-spoke constellation. Without this, you would just have a list of model calls. With this, you have a controlled workflow whose next action is selected deterministically from state.\n","\n","The `hub_router(state)` function does three checks in order. First, it checks refusal. If the system has entered a refusal state, it records a termination reason and stops routing. Second, it checks bounded execution: if steps have reached the maximum, it terminates with `MAX_STEPS_REACHED`, clears pending work, and forces the workflow to move toward aggregation. This is a safety control. Third, it checks the pending section queue. If no sections remain, the hub sets `current_section` to None, which signals that we should proceed to aggregation.\n","\n","If sections remain, the hub pops the next section from the front of the list. This is deterministic: it is not “choose based on what sounds important.” It is “follow the queue.” That makes runs predictable and easy to explain.\n","\n","The two routing functions are the other key element. `route_from_hub(state)` decides whether we go to a writer node or to aggregation. If `current_section` is set, it returns the corresponding writer node name. If not, it routes to aggregate. This is conditional routing via LangGraph only, satisfying the project constraint. `route_after_spoke(state)` does the same after a writer finishes: if there is more work and we are below the step bound, go back to the hub; otherwise go to aggregate.\n","\n","This design is critical for professional interpretability. The model is not deciding the process; the graph is. Routing depends on explicit fields: `pending_sections`, `current_section`, `steps`, and `max_steps`. You can audit this after the run by inspecting final_state.json.\n","\n","When presenting to your bosses, this cell is your strongest architectural argument. You can say: “We have a coordinator node that assigns work sections one by one. The AI is not ‘thinking’ about what to do next; it is assigned tasks by a deterministic router. That makes the workflow governable.”\n","\n","In short, Cell 5 turns a set of drafting agents into an orchestrated system with explicit control flow, bounded behavior, and predictable sequencing.\n"],"metadata":{"id":"WY3hc3q2iSKx"}},{"cell_type":"markdown","source":["###5.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"fzeMhbzgiShs"}},{"cell_type":"code","source":["# CELL 5/10 — Hub router node + deterministic routing decisions (hub-and-spoke constellation)\n","\n","def hub_router(state: PitchbookState) -> PitchbookState:\n","    if state.get(\"refusal\"):\n","        state[\"termination_reason\"] = state.get(\"termination_reason\") or \"REFUSAL\"\n","        return state\n","\n","    steps = int(state.get(\"steps\", 0))\n","    max_steps = int(state.get(\"max_steps\", 0))\n","    if steps >= max_steps:\n","        state[\"termination_reason\"] = \"MAX_STEPS_REACHED\"\n","        # clear pending to force aggregator/end\n","        state[\"pending_sections\"] = []\n","        state[\"current_section\"] = None\n","        return state\n","\n","    pending = list(state.get(\"pending_sections\", []))\n","    if not pending:\n","        state[\"current_section\"] = None\n","        return state\n","\n","    # Deterministic selection: pop from front\n","    next_section = pending.pop(0)\n","    state[\"pending_sections\"] = pending\n","    state[\"current_section\"] = next_section\n","    return state\n","\n","HubNode = AgentNode(\"HUB_ROUTER\", hub_router)\n","\n","def route_from_hub(state: PitchbookState) -> str:\n","    \"\"\"\n","    Conditional routing via LangGraph only.\n","    \"\"\"\n","    if state.get(\"refusal\"):\n","        return \"AGGREGATE\"\n","    if state.get(\"current_section\") is None:\n","        return \"AGGREGATE\"\n","    return f\"WRITE_{state['current_section']}\"\n","\n","def route_after_spoke(state: PitchbookState) -> str:\n","    \"\"\"\n","    After each spoke, return to hub unless done.\n","    \"\"\"\n","    if state.get(\"refusal\"):\n","        return \"AGGREGATE\"\n","    pending = state.get(\"pending_sections\", [])\n","    steps = int(state.get(\"steps\", 0))\n","    if (not pending) or (steps >= int(state.get(\"max_steps\", 0))):\n","        return \"AGGREGATE\"\n","    return \"HUB_ROUTER\"\n","\n","print(\"Hub routing functions ready.\")\n"],"metadata":{"id":"t3ZYvRswjUKQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771501261971,"user_tz":360,"elapsed":7,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"5ad70989-3db3-4e4b-9035-d847dd06f099"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Hub routing functions ready.\n"]}]},{"cell_type":"markdown","source":["##6.AGGREGATOR"],"metadata":{"id":"3qMrcRA8iVGP"}},{"cell_type":"markdown","source":["###6.1.OVERVIEW"],"metadata":{"id":"3Ak9KwaQiXOz"}},{"cell_type":"markdown","source":["**Cell 6 — Aggregation and graph construction (turning components into a runnable topology)**\n","\n","Cell 6 is where we assemble the entire architecture into a LangGraph workflow. Up to this point, we have the building blocks: state schema, drafting spokes, and routing logic. Now we wire them into a graph with an explicit entry point, conditional edges, and an explicit END.\n","\n","First, we define `aggregate_pitchbook(state)`. This node is the “assembler,” analogous to an associate who collects section drafts and compiles them into a coherent document. It takes the `section_outputs` dictionary and stitches sections together in a standard order. It also adds a header with the company name, mandate, audience, and a “Not verified” disclaimer. This is the correct professional posture: the draft is a working document, not a final truth artifact.\n","\n","The aggregator also produces a consolidated gap list. It collects gaps from each section and prefixes them with section names. This is extremely practical: it becomes the actionable checklist for the team—what to confirm, what to source, what to refine before client delivery. In many workflows, a high-quality gap list is worth nearly as much as the draft itself.\n","\n","Then we build the graph. `StateGraph(PitchbookState)` enforces that nodes consume and produce the declared state type. We add nodes: the hub, each writer, and the aggregator. We set the entry point to the hub.\n","\n","The core wiring is conditional routing. From the hub, we add conditional edges using `route_from_hub`, mapping each possible return string to its node destination. From each writer, we add conditional edges using `route_after_spoke` back to the hub or to aggregate. This satisfies the constraint that conditional routing must be done through LangGraph, not ad hoc if-statements outside the graph.\n","\n","Finally, we connect AGGREGATE to END with a direct edge. This gives us an explicit end node. That is not just a formal requirement; it is an operational guarantee that the workflow terminates cleanly.\n","\n","When you show the committee the Mermaid diagram, Cell 6 is the code that makes that diagram true. The diagram is not a conceptual sketch; it is generated from the compiled graph.\n","\n","In short, Cell 6 is where the system becomes “real”: a concrete topology whose behavior is constrained by state and whose output includes both a pitchbook draft and an explicit gap map.\n"],"metadata":{"id":"Fnhd7Fwaicyv"}},{"cell_type":"markdown","source":["###6.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"0jFxOakbido7"}},{"cell_type":"code","source":["# CELL 6/10 — Aggregator + graph build (StateGraph), bounded loop, explicit END\n","\n","def aggregate_pitchbook(state: PitchbookState) -> PitchbookState:\n","    outputs = state.get(\"section_outputs\", {})\n","    order = [str(s) for s in DEFAULT_SECTIONS]\n","    parts: List[str] = []\n","    parts.append(f\"PITCHBOOK DRAFT — {state.get('company_name','')} ({state.get('ticker','')})\")\n","    parts.append(f\"Mandate: {state.get('mandate_type')} | Audience: {state.get('audience')} | Style: {state.get('style')}\")\n","    parts.append(\"Verification: Not verified (synthetic demo)\")\n","    parts.append(\"\")\n","\n","    for sec in order:\n","        if sec in outputs:\n","            parts.append(outputs[sec].strip())\n","            parts.append(\"\\n\" + (\"-\" * 72) + \"\\n\")\n","        else:\n","            parts.append(f\"TITLE: {sec}\\nCONTENT:\\n- [Missing section output]\\nGAPS:\\n- Not produced\\nNEXT:\\n- Produce this section\\n\")\n","            parts.append(\"\\n\" + (\"-\" * 72) + \"\\n\")\n","\n","    # Consolidated gaps\n","    gap_map = state.get(\"section_gaps\", {})\n","    all_gaps: List[str] = []\n","    for sec in order:\n","        for g in gap_map.get(sec, []):\n","            all_gaps.append(f\"{sec}: {g}\")\n","    if not all_gaps:\n","        all_gaps = [\"None\"]\n","\n","    parts.append(\"CONSOLIDATED GAPS (cross-section)\")\n","    for g in all_gaps:\n","        parts.append(f\"- {g}\")\n","\n","    state[\"assembled_pitchbook\"] = \"\\n\".join(parts).strip()\n","    state[\"termination_reason\"] = state.get(\"termination_reason\") or \"COMPLETED\"\n","    return state\n","\n","AggNode = AgentNode(\"AGGREGATE\", aggregate_pitchbook)\n","\n","graph = StateGraph(PitchbookState)\n","\n","graph.add_node(\"HUB_ROUTER\", HubNode)\n","for sec, node in SPOKES.items():\n","    graph.add_node(node.name, node)\n","graph.add_node(\"AGGREGATE\", AggNode)\n","\n","graph.set_entry_point(\"HUB_ROUTER\")\n","\n","# Conditional routing from hub to the selected spoke (or aggregator)\n","cond_map_from_hub = {f\"WRITE_{sec}\": f\"WRITE_{sec}\" for sec in DEFAULT_SECTIONS}\n","cond_map_from_hub[\"AGGREGATE\"] = \"AGGREGATE\"\n","graph.add_conditional_edges(\"HUB_ROUTER\", route_from_hub, cond_map_from_hub)\n","\n","# After each spoke, go back to hub or aggregate\n","cond_map_after_spoke = {\"HUB_ROUTER\": \"HUB_ROUTER\", \"AGGREGATE\": \"AGGREGATE\"}\n","for sec in DEFAULT_SECTIONS:\n","    graph.add_conditional_edges(f\"WRITE_{sec}\", route_after_spoke, cond_map_after_spoke)\n","\n","# Aggregate then END (explicit)\n","graph.add_edge(\"AGGREGATE\", END)\n","\n","compiled = graph.compile()\n","print(\"Graph compiled. Nodes:\", compiled.get_graph().nodes)\n"],"metadata":{"id":"HR3pyRxjjV3p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771501261993,"user_tz":360,"elapsed":18,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"36379f0c-2802-4969-dbe3-75e4773af4f2"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Graph compiled. Nodes: {'__start__': Node(id='__start__', name='__start__', data=<class 'langchain_core.utils.pydantic.LangGraphInput'>, metadata=None), 'HUB_ROUTER': Node(id='HUB_ROUTER', name='HUB_ROUTER', data=HUB_ROUTER(tags=None, recurse=True, func_accepts_config=False, func_accepts={'writer': False, 'store': False}), metadata=None), 'WRITE_EXEC_SUMMARY': Node(id='WRITE_EXEC_SUMMARY', name='WRITE_EXEC_SUMMARY', data=WRITE_EXEC_SUMMARY(tags=None, recurse=True, func_accepts_config=False, func_accepts={'writer': False, 'store': False}), metadata=None), 'WRITE_COMPANY_OVERVIEW': Node(id='WRITE_COMPANY_OVERVIEW', name='WRITE_COMPANY_OVERVIEW', data=WRITE_COMPANY_OVERVIEW(tags=None, recurse=True, func_accepts_config=False, func_accepts={'writer': False, 'store': False}), metadata=None), 'WRITE_MARKET_OVERVIEW': Node(id='WRITE_MARKET_OVERVIEW', name='WRITE_MARKET_OVERVIEW', data=WRITE_MARKET_OVERVIEW(tags=None, recurse=True, func_accepts_config=False, func_accepts={'writer': False, 'store': False}), metadata=None), 'WRITE_INVESTMENT_THESIS': Node(id='WRITE_INVESTMENT_THESIS', name='WRITE_INVESTMENT_THESIS', data=WRITE_INVESTMENT_THESIS(tags=None, recurse=True, func_accepts_config=False, func_accepts={'writer': False, 'store': False}), metadata=None), 'WRITE_TRADING_COMPS': Node(id='WRITE_TRADING_COMPS', name='WRITE_TRADING_COMPS', data=WRITE_TRADING_COMPS(tags=None, recurse=True, func_accepts_config=False, func_accepts={'writer': False, 'store': False}), metadata=None), 'WRITE_TRANSACTION_COMPS': Node(id='WRITE_TRANSACTION_COMPS', name='WRITE_TRANSACTION_COMPS', data=WRITE_TRANSACTION_COMPS(tags=None, recurse=True, func_accepts_config=False, func_accepts={'writer': False, 'store': False}), metadata=None), 'WRITE_FINANCIAL_SUMMARY': Node(id='WRITE_FINANCIAL_SUMMARY', name='WRITE_FINANCIAL_SUMMARY', data=WRITE_FINANCIAL_SUMMARY(tags=None, recurse=True, func_accepts_config=False, func_accepts={'writer': False, 'store': False}), metadata=None), 'WRITE_VALUATION': Node(id='WRITE_VALUATION', name='WRITE_VALUATION', data=WRITE_VALUATION(tags=None, recurse=True, func_accepts_config=False, func_accepts={'writer': False, 'store': False}), metadata=None), 'WRITE_RISKS_AND_MITIGANTS': Node(id='WRITE_RISKS_AND_MITIGANTS', name='WRITE_RISKS_AND_MITIGANTS', data=WRITE_RISKS_AND_MITIGANTS(tags=None, recurse=True, func_accepts_config=False, func_accepts={'writer': False, 'store': False}), metadata=None), 'WRITE_DEAL_STRUCTURE': Node(id='WRITE_DEAL_STRUCTURE', name='WRITE_DEAL_STRUCTURE', data=WRITE_DEAL_STRUCTURE(tags=None, recurse=True, func_accepts_config=False, func_accepts={'writer': False, 'store': False}), metadata=None), 'AGGREGATE': Node(id='AGGREGATE', name='AGGREGATE', data=AGGREGATE(tags=None, recurse=True, func_accepts_config=False, func_accepts={'writer': False, 'store': False}), metadata=None), '__end__': Node(id='__end__', name='__end__', data=<class 'langchain_core.utils.pydantic.LangGraphOutput'>, metadata=None)}\n"]}]},{"cell_type":"markdown","source":["##7.VISUALIZATION"],"metadata":{"id":"hnqxNRdOigzp"}},{"cell_type":"markdown","source":["###7.1.OVERVIEW"],"metadata":{"id":"9N3hP5ksikQZ"}},{"cell_type":"markdown","source":["**Cell 7 — Visualization as a governance artifact (white background, black fonts, exact topology)**\n","\n","Cell 7 is not “nice to have.” In this series, visualization is mandatory because the diagram is part of the learning and governance package. A committee needs to see the shape of the system: what nodes exist, how work flows, where loops are, and where the workflow ends. A graph picture makes this understandable immediately, even to people who do not read Python.\n","\n","This cell uses a hardened approach because Colab environments are inconsistent, and Mermaid rendering can fail due to ESM loading issues or API drift. The first step is extracting the Mermaid code. The helper `_extract_mermaid_any(obj)` tries multiple extraction paths because LangGraph versions can vary in how they expose `draw_mermaid()`. This is defensive engineering: we want the notebook to work reliably in a classroom, not just on one machine.\n","\n","Then `display_langgraph_mermaid(...)` renders the diagram using Mermaid ESM pinned to a known version (10.6.1). Pinning matters: rendering changes can break styling and layout. We select Mermaid’s “base” theme and override theme variables to enforce a white background and black text. After rendering, we also inject a small SVG style block to force white fills and black strokes even if Mermaid theme variables drift. The result is consistent: white canvas, black fonts, readable in committee rooms and printed PDFs.\n","\n","The cell also includes a fallback: the Mermaid source is available under “Show Mermaid source.” This is helpful when debugging topology mismatches or when validating that the diagram matches what we believe we built. In governance terms, this transparency is a control: we can inspect the graph as code, not just as an image.\n","\n","For your bosses, the message is: “This diagram is the system contract.” If someone asks, “Can the system skip valuation and go straight to risks?” you can point to the edges. If someone asks, “How do we ensure it terminates?” you show the explicit END. If someone asks, “What work is parallelizable later?” you show the spokes.\n","\n","In short, Cell 7 turns the architecture into something reviewable by non-programmers. It is a professional communication tool and a compliance-friendly artifact. In a bank, anything that reduces ambiguity and increases shared understanding has immediate value.\n"],"metadata":{"id":"MzaMhrkuimFO"}},{"cell_type":"markdown","source":["###7.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"epotbIpqimYU"}},{"cell_type":"code","source":["# CELL 7/10 — Visualization (mandatory): WHITE background + BLACK fonts (Colab hardened)\n","\n","def _extract_mermaid_any(obj) -> str:\n","    last_err = None\n","    try:\n","        g = obj.get_graph()\n","        try:\n","            return g.draw_mermaid()\n","        except TypeError:\n","            return g.draw_mermaid(xray=False)\n","    except Exception as e:\n","        last_err = e\n","\n","    try:\n","        g = getattr(obj, \"graph\", None)\n","        if g is not None and hasattr(g, \"get_graph\"):\n","            gg = g.get_graph()\n","            try:\n","                return gg.draw_mermaid()\n","            except TypeError:\n","                return gg.draw_mermaid(xray=False)\n","    except Exception as e:\n","        last_err = e\n","\n","    try:\n","        if hasattr(obj, \"draw_mermaid\"):\n","            return obj.draw_mermaid()\n","    except Exception as e:\n","        last_err = e\n","\n","    raise RuntimeError(f\"Could not extract Mermaid from graph object. Last error: {last_err!r}\")\n","\n","def display_langgraph_mermaid(compiled_or_graph, *, mermaid_version: str = \"10.6.1\") -> None:\n","    try:\n","        mermaid_code = _extract_mermaid_any(compiled_or_graph)\n","    except Exception as e:\n","        display(HTML(f\"<pre style='white-space:pre-wrap'>Mermaid extraction failed: {e!r}</pre>\"))\n","        return\n","\n","    diagram_id = f\"mermaid-diagram-{uuid.uuid4().hex[:10]}\"\n","    esc = (mermaid_code.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\"))\n","\n","    html = f\"\"\"\n","<div style=\"border:1px solid rgba(0,0,0,0.15); border-radius:12px; padding:12px; background:#ffffff;\">\n","  <div style=\"display:flex; align-items:center; justify-content:space-between; gap:12px; margin-bottom:8px;\">\n","    <div style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace;\n","                color:#111111; font-size:12px; opacity:0.9;\">\n","      LangGraph topology (Mermaid {mermaid_version})\n","    </div>\n","    <div style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace;\n","                color:#333333; font-size:11px; opacity:0.85;\">\n","      If blank: Runtime → Restart runtime then rerun cells 1→7\n","    </div>\n","  </div>\n","\n","  <div id=\"{diagram_id}-container\">\n","    <pre class=\"mermaid\" id=\"{diagram_id}\" style=\"background:transparent; margin:0; color:#111111;\">{esc}</pre>\n","  </div>\n","\n","  <details style=\"margin-top:10px;\">\n","    <summary style=\"cursor:pointer; color:#111111; font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace; font-size:11px;\">\n","      Show Mermaid source\n","    </summary>\n","    <pre style=\"white-space:pre-wrap; background:#f6f7fb; padding:10px; border-radius:10px; border:1px solid rgba(0,0,0,0.12);\n","                color:#111111; font-size:11px; margin-top:8px;\">{esc}</pre>\n","  </details>\n","</div>\n","\n","<script type=\"module\">\n","  import mermaid from \"https://cdn.jsdelivr.net/npm/mermaid@{mermaid_version}/dist/mermaid.esm.min.mjs\";\n","  const target = document.getElementById(\"{diagram_id}\");\n","  const container = document.getElementById(\"{diagram_id}-container\");\n","\n","  try {{\n","    mermaid.initialize({{\n","      startOnLoad: false,\n","      securityLevel: \"strict\",\n","      theme: \"base\",\n","      themeVariables: {{\n","        background: \"#ffffff\",\n","        primaryColor: \"#ffffff\",\n","        primaryTextColor: \"#111111\",\n","        primaryBorderColor: \"#111111\",\n","        lineColor: \"#111111\",\n","        secondaryColor: \"#ffffff\",\n","        tertiaryColor: \"#ffffff\",\n","        fontFamily: \"ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, Liberation Mono, Courier New, monospace\",\n","        fontSize: \"12px\"\n","      }},\n","      flowchart: {{ curve: \"basis\" }},\n","    }});\n","\n","    const code = target.textContent;\n","    const result = await mermaid.render(\"{diagram_id}-svg\", code);\n","    target.outerHTML = result.svg;\n","\n","    // Force white canvas + black strokes even if Mermaid theme drifts\n","    const svgEl = container.querySelector(\"svg\");\n","    if (svgEl) {{\n","      svgEl.style.background = \"#ffffff\";\n","      const style = document.createElementNS(\"http://www.w3.org/2000/svg\", \"style\");\n","      style.textContent = `\n","        svg {{ background: #ffffff !important; }}\n","        .node rect, .node polygon, .node circle, .node ellipse {{\n","          fill: #ffffff !important;\n","          stroke: #111111 !important;\n","        }}\n","        .edgePath path, .flowchart-link {{\n","          stroke: #111111 !important;\n","        }}\n","        text {{\n","          fill: #111111 !important;\n","        }}\n","      `;\n","      svgEl.appendChild(style);\n","    }}\n","  }} catch (err) {{\n","    const msg = \"Mermaid render failed: \" + err;\n","    container.innerHTML = \"<pre style='white-space:pre-wrap; color:#111111; background:#fff2f2; padding:10px; border-radius:10px; border:1px solid rgba(0,0,0,0.15);'>\" + msg + \"</pre>\";\n","  }}\n","</script>\n","\"\"\"\n","    display(HTML(html))\n","\n","# run\n","display_langgraph_mermaid(compiled, mermaid_version=CONFIG[\"mermaid_version\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"JcL1kseYtxgB","executionInfo":{"status":"ok","timestamp":1771501463752,"user_tz":360,"elapsed":292,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"583c7810-c34a-4866-e4b1-6cef1d0a52a6"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<div style=\"border:1px solid rgba(0,0,0,0.15); border-radius:12px; padding:12px; background:#ffffff;\">\n","  <div style=\"display:flex; align-items:center; justify-content:space-between; gap:12px; margin-bottom:8px;\">\n","    <div style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace;\n","                color:#111111; font-size:12px; opacity:0.9;\">\n","      LangGraph topology (Mermaid 10.6.1)\n","    </div>\n","    <div style=\"font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace;\n","                color:#333333; font-size:11px; opacity:0.85;\">\n","      If blank: Runtime → Restart runtime then rerun cells 1→7\n","    </div>\n","  </div>\n","\n","  <div id=\"mermaid-diagram-28d808eb7c-container\">\n","    <pre class=\"mermaid\" id=\"mermaid-diagram-28d808eb7c\" style=\"background:transparent; margin:0; color:#111111;\">%%{init: {'flowchart': {'curve': 'linear'}}}%%\n","graph TD;\n","\t__start__([&lt;p&gt;__start__&lt;/p&gt;]):::first\n","\tHUB_ROUTER(HUB_ROUTER)\n","\tWRITE_EXEC_SUMMARY(WRITE_EXEC_SUMMARY)\n","\tWRITE_COMPANY_OVERVIEW(WRITE_COMPANY_OVERVIEW)\n","\tWRITE_MARKET_OVERVIEW(WRITE_MARKET_OVERVIEW)\n","\tWRITE_INVESTMENT_THESIS(WRITE_INVESTMENT_THESIS)\n","\tWRITE_TRADING_COMPS(WRITE_TRADING_COMPS)\n","\tWRITE_TRANSACTION_COMPS(WRITE_TRANSACTION_COMPS)\n","\tWRITE_FINANCIAL_SUMMARY(WRITE_FINANCIAL_SUMMARY)\n","\tWRITE_VALUATION(WRITE_VALUATION)\n","\tWRITE_RISKS_AND_MITIGANTS(WRITE_RISKS_AND_MITIGANTS)\n","\tWRITE_DEAL_STRUCTURE(WRITE_DEAL_STRUCTURE)\n","\tAGGREGATE(AGGREGATE)\n","\t__end__([&lt;p&gt;__end__&lt;/p&gt;]):::last\n","\tAGGREGATE --&gt; __end__;\n","\t__start__ --&gt; HUB_ROUTER;\n","\tHUB_ROUTER -.-&gt; WRITE_EXEC_SUMMARY;\n","\tHUB_ROUTER -.-&gt; WRITE_COMPANY_OVERVIEW;\n","\tHUB_ROUTER -.-&gt; WRITE_MARKET_OVERVIEW;\n","\tHUB_ROUTER -.-&gt; WRITE_INVESTMENT_THESIS;\n","\tHUB_ROUTER -.-&gt; WRITE_TRADING_COMPS;\n","\tHUB_ROUTER -.-&gt; WRITE_TRANSACTION_COMPS;\n","\tHUB_ROUTER -.-&gt; WRITE_FINANCIAL_SUMMARY;\n","\tHUB_ROUTER -.-&gt; WRITE_VALUATION;\n","\tHUB_ROUTER -.-&gt; WRITE_RISKS_AND_MITIGANTS;\n","\tHUB_ROUTER -.-&gt; WRITE_DEAL_STRUCTURE;\n","\tHUB_ROUTER -.-&gt; AGGREGATE;\n","\tWRITE_EXEC_SUMMARY -.-&gt; HUB_ROUTER;\n","\tWRITE_EXEC_SUMMARY -.-&gt; AGGREGATE;\n","\tWRITE_COMPANY_OVERVIEW -.-&gt; HUB_ROUTER;\n","\tWRITE_COMPANY_OVERVIEW -.-&gt; AGGREGATE;\n","\tWRITE_MARKET_OVERVIEW -.-&gt; HUB_ROUTER;\n","\tWRITE_MARKET_OVERVIEW -.-&gt; AGGREGATE;\n","\tWRITE_INVESTMENT_THESIS -.-&gt; HUB_ROUTER;\n","\tWRITE_INVESTMENT_THESIS -.-&gt; AGGREGATE;\n","\tWRITE_TRADING_COMPS -.-&gt; HUB_ROUTER;\n","\tWRITE_TRADING_COMPS -.-&gt; AGGREGATE;\n","\tWRITE_TRANSACTION_COMPS -.-&gt; HUB_ROUTER;\n","\tWRITE_TRANSACTION_COMPS -.-&gt; AGGREGATE;\n","\tWRITE_FINANCIAL_SUMMARY -.-&gt; HUB_ROUTER;\n","\tWRITE_FINANCIAL_SUMMARY -.-&gt; AGGREGATE;\n","\tWRITE_VALUATION -.-&gt; HUB_ROUTER;\n","\tWRITE_VALUATION -.-&gt; AGGREGATE;\n","\tWRITE_RISKS_AND_MITIGANTS -.-&gt; HUB_ROUTER;\n","\tWRITE_RISKS_AND_MITIGANTS -.-&gt; AGGREGATE;\n","\tWRITE_DEAL_STRUCTURE -.-&gt; HUB_ROUTER;\n","\tWRITE_DEAL_STRUCTURE -.-&gt; AGGREGATE;\n","\tclassDef default fill:#f2f0ff,line-height:1.2\n","\tclassDef first fill-opacity:0\n","\tclassDef last fill:#bfb6fc\n","</pre>\n","  </div>\n","\n","  <details style=\"margin-top:10px;\">\n","    <summary style=\"cursor:pointer; color:#111111; font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, 'Liberation Mono', 'Courier New', monospace; font-size:11px;\">\n","      Show Mermaid source\n","    </summary>\n","    <pre style=\"white-space:pre-wrap; background:#f6f7fb; padding:10px; border-radius:10px; border:1px solid rgba(0,0,0,0.12);\n","                color:#111111; font-size:11px; margin-top:8px;\">%%{init: {'flowchart': {'curve': 'linear'}}}%%\n","graph TD;\n","\t__start__([&lt;p&gt;__start__&lt;/p&gt;]):::first\n","\tHUB_ROUTER(HUB_ROUTER)\n","\tWRITE_EXEC_SUMMARY(WRITE_EXEC_SUMMARY)\n","\tWRITE_COMPANY_OVERVIEW(WRITE_COMPANY_OVERVIEW)\n","\tWRITE_MARKET_OVERVIEW(WRITE_MARKET_OVERVIEW)\n","\tWRITE_INVESTMENT_THESIS(WRITE_INVESTMENT_THESIS)\n","\tWRITE_TRADING_COMPS(WRITE_TRADING_COMPS)\n","\tWRITE_TRANSACTION_COMPS(WRITE_TRANSACTION_COMPS)\n","\tWRITE_FINANCIAL_SUMMARY(WRITE_FINANCIAL_SUMMARY)\n","\tWRITE_VALUATION(WRITE_VALUATION)\n","\tWRITE_RISKS_AND_MITIGANTS(WRITE_RISKS_AND_MITIGANTS)\n","\tWRITE_DEAL_STRUCTURE(WRITE_DEAL_STRUCTURE)\n","\tAGGREGATE(AGGREGATE)\n","\t__end__([&lt;p&gt;__end__&lt;/p&gt;]):::last\n","\tAGGREGATE --&gt; __end__;\n","\t__start__ --&gt; HUB_ROUTER;\n","\tHUB_ROUTER -.-&gt; WRITE_EXEC_SUMMARY;\n","\tHUB_ROUTER -.-&gt; WRITE_COMPANY_OVERVIEW;\n","\tHUB_ROUTER -.-&gt; WRITE_MARKET_OVERVIEW;\n","\tHUB_ROUTER -.-&gt; WRITE_INVESTMENT_THESIS;\n","\tHUB_ROUTER -.-&gt; WRITE_TRADING_COMPS;\n","\tHUB_ROUTER -.-&gt; WRITE_TRANSACTION_COMPS;\n","\tHUB_ROUTER -.-&gt; WRITE_FINANCIAL_SUMMARY;\n","\tHUB_ROUTER -.-&gt; WRITE_VALUATION;\n","\tHUB_ROUTER -.-&gt; WRITE_RISKS_AND_MITIGANTS;\n","\tHUB_ROUTER -.-&gt; WRITE_DEAL_STRUCTURE;\n","\tHUB_ROUTER -.-&gt; AGGREGATE;\n","\tWRITE_EXEC_SUMMARY -.-&gt; HUB_ROUTER;\n","\tWRITE_EXEC_SUMMARY -.-&gt; AGGREGATE;\n","\tWRITE_COMPANY_OVERVIEW -.-&gt; HUB_ROUTER;\n","\tWRITE_COMPANY_OVERVIEW -.-&gt; AGGREGATE;\n","\tWRITE_MARKET_OVERVIEW -.-&gt; HUB_ROUTER;\n","\tWRITE_MARKET_OVERVIEW -.-&gt; AGGREGATE;\n","\tWRITE_INVESTMENT_THESIS -.-&gt; HUB_ROUTER;\n","\tWRITE_INVESTMENT_THESIS -.-&gt; AGGREGATE;\n","\tWRITE_TRADING_COMPS -.-&gt; HUB_ROUTER;\n","\tWRITE_TRADING_COMPS -.-&gt; AGGREGATE;\n","\tWRITE_TRANSACTION_COMPS -.-&gt; HUB_ROUTER;\n","\tWRITE_TRANSACTION_COMPS -.-&gt; AGGREGATE;\n","\tWRITE_FINANCIAL_SUMMARY -.-&gt; HUB_ROUTER;\n","\tWRITE_FINANCIAL_SUMMARY -.-&gt; AGGREGATE;\n","\tWRITE_VALUATION -.-&gt; HUB_ROUTER;\n","\tWRITE_VALUATION -.-&gt; AGGREGATE;\n","\tWRITE_RISKS_AND_MITIGANTS -.-&gt; HUB_ROUTER;\n","\tWRITE_RISKS_AND_MITIGANTS -.-&gt; AGGREGATE;\n","\tWRITE_DEAL_STRUCTURE -.-&gt; HUB_ROUTER;\n","\tWRITE_DEAL_STRUCTURE -.-&gt; AGGREGATE;\n","\tclassDef default fill:#f2f0ff,line-height:1.2\n","\tclassDef first fill-opacity:0\n","\tclassDef last fill:#bfb6fc\n","</pre>\n","  </details>\n","</div>\n","\n","<script type=\"module\">\n","  import mermaid from \"https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs\";\n","  const target = document.getElementById(\"mermaid-diagram-28d808eb7c\");\n","  const container = document.getElementById(\"mermaid-diagram-28d808eb7c-container\");\n","\n","  try {\n","    mermaid.initialize({\n","      startOnLoad: false,\n","      securityLevel: \"strict\",\n","      theme: \"base\",\n","      themeVariables: {\n","        background: \"#ffffff\",\n","        primaryColor: \"#ffffff\",\n","        primaryTextColor: \"#111111\",\n","        primaryBorderColor: \"#111111\",\n","        lineColor: \"#111111\",\n","        secondaryColor: \"#ffffff\",\n","        tertiaryColor: \"#ffffff\",\n","        fontFamily: \"ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, Liberation Mono, Courier New, monospace\",\n","        fontSize: \"12px\"\n","      },\n","      flowchart: { curve: \"basis\" },\n","    });\n","\n","    const code = target.textContent;\n","    const result = await mermaid.render(\"mermaid-diagram-28d808eb7c-svg\", code);\n","    target.outerHTML = result.svg;\n","\n","    // Force white canvas + black strokes even if Mermaid theme drifts\n","    const svgEl = container.querySelector(\"svg\");\n","    if (svgEl) {\n","      svgEl.style.background = \"#ffffff\";\n","      const style = document.createElementNS(\"http://www.w3.org/2000/svg\", \"style\");\n","      style.textContent = `\n","        svg { background: #ffffff !important; }\n","        .node rect, .node polygon, .node circle, .node ellipse {\n","          fill: #ffffff !important;\n","          stroke: #111111 !important;\n","        }\n","        .edgePath path, .flowchart-link {\n","          stroke: #111111 !important;\n","        }\n","        text {\n","          fill: #111111 !important;\n","        }\n","      `;\n","      svgEl.appendChild(style);\n","    }\n","  } catch (err) {\n","    const msg = \"Mermaid render failed: \" + err;\n","    container.innerHTML = \"<pre style='white-space:pre-wrap; color:#111111; background:#fff2f2; padding:10px; border-radius:10px; border:1px solid rgba(0,0,0,0.15);'>\" + msg + \"</pre>\";\n","  }\n","</script>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["##8.EXECUTION"],"metadata":{"id":"BLus531lirIL"}},{"cell_type":"markdown","source":["###8.1.OVERVIEW"],"metadata":{"id":"x0j2JYTnixuf"}},{"cell_type":"markdown","source":["**Cell 8 — Execution: running a full pitchbook draft from a clean initial state**\n","\n","Cell 8 is where the notebook demonstrates the system end-to-end. It creates a run instance, builds an initial state, executes the compiled graph, and prints a compact summary of what happened.\n","\n","The run begins by generating a unique `run_id`. This is important because we want every run to be traceable. In production, run IDs link outputs to deals, versions, and approvals. Then we build the synthetic input pack deterministically. The key pedagogical idea is: the workflow is not dependent on external data availability. It is dependent on a structured input pack. That makes it easier to explain and easier to validate.\n","\n","Next we define `init_state`. This is the single source of truth for what the system knows when it starts. It includes the mandate type (e.g., M&A sell-side), the audience (IC), the style (institutional), and basic company descriptors. It includes the `pending_sections` list preloaded with the pitchbook section blueprint. It sets `current_section` to None, and initializes empty dictionaries for outputs and gaps. It also sets governance flags: refusal false, termination reason none. Finally, it sets bounded loop controls: `steps` starts at 0 and `max_steps` is set to a value that allows the section list plus a small overhead.\n","\n","This is how we keep autonomy bounded: the system cannot keep drafting forever. Even if something goes wrong, it will stop and report the reason.\n","\n","Then we call `compiled.invoke(init_state)`. From here, the graph runs exactly as defined: hub assigns a section, a writer drafts it, the state updates, and control returns to the hub. This repeats until pending sections are empty or max steps is reached. Finally, the aggregator assembles the pitchbook and the run terminates.\n","\n","The printed output is deliberately modest: it prints termination reason, steps executed, and how many sections were completed. It then prints the first portion of the assembled pitchbook so you can quickly see that the output is coherent without flooding the notebook.\n","\n","When explaining to senior stakeholders, Cell 8 is your “proof of life.” It demonstrates that the system is not hypothetical. It runs in a few minutes, produces a structured draft, and surfaces gaps. It shows the practical benefit: analysts can start from a coherent skeleton rather than a blank page, and the deal team gets an explicit checklist of missing information.\n","\n","This is the moment you can say: “We can generate a first-pass pitchbook draft in a controlled way, with bounded behavior, from a curated input pack.”\n"],"metadata":{"id":"CGJ3TUQwi2JO"}},{"cell_type":"markdown","source":["###8.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"SCv1wHOni2cy"}},{"cell_type":"code","source":["# CELL 8/10 — Execute: sample IB pitchbook run (hub-and-spoke constellation)\n","\n","run_id = f\"run_{uuid.uuid4().hex[:10]}\"\n","seed_pack = deterministic_synthetic_input_pack(seed=7)\n","\n","init_state: PitchbookState = {\n","    \"run_id\": run_id,\n","    \"ts_utc\": utc_now_iso(),\n","    \"max_steps\": len(DEFAULT_SECTIONS) + 3,  # bounded: allow small overhead; loop remains hard-bounded\n","    \"steps\": 0,\n","\n","    \"mandate_type\": \"M&A_SELLSIDE\",\n","    \"audience\": \"IC\",\n","    \"style\": \"INSTITUTIONAL\",\n","    \"company_name\": \"BlueLedger\",\n","    \"ticker\": \"BLDG\",\n","    \"sector\": \"B2B FinTech / Finance Automation\",\n","    \"geography\": \"North America\",\n","\n","    \"input_pack\": seed_pack,\n","\n","    \"pending_sections\": list(DEFAULT_SECTIONS),\n","    \"current_section\": None,\n","    \"section_outputs\": {},\n","    \"section_gaps\": {},\n","\n","    \"assembled_pitchbook\": \"\",\n","    \"refusal\": False,\n","    \"refusal_reason\": None,\n","    \"termination_reason\": None,\n","}\n","\n","final_state: PitchbookState = compiled.invoke(init_state)\n","\n","print(\"DONE:\", {\n","    \"run_id\": final_state.get(\"run_id\"),\n","    \"termination_reason\": final_state.get(\"termination_reason\"),\n","    \"steps_executed\": final_state.get(\"steps\"),\n","    \"sections_completed\": len(final_state.get(\"section_outputs\", {})),\n","})\n","print(\"\\n--- PITCHBOOK (first 1200 chars) ---\\n\")\n","print(final_state.get(\"assembled_pitchbook\", \"\")[:1200])\n"],"metadata":{"id":"s8BmlsxYiw1_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771501605021,"user_tz":360,"elapsed":95383,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"37e13775-94c7-43de-ef07-1df86ffea710"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["DONE: {'run_id': 'run_db0ac89110', 'termination_reason': 'COMPLETED', 'steps_executed': 10, 'sections_completed': 10}\n","\n","--- PITCHBOOK (first 1200 chars) ---\n","\n","PITCHBOOK DRAFT — BlueLedger (BLDG)\n","Mandate: M&A_SELLSIDE | Audience: IC | Style: INSTITUTIONAL\n","Verification: Not verified (synthetic demo)\n","\n","# EXECUTIVE SUMMARY\n","\n","---\n","\n","## BlueLedger: B2B FinTech Finance Automation Platform – Sell-Side M&A Opportunity\n","\n","**Investment Thesis**\n","\n","- **Market Position:** BlueLedger is a North American B2B FinTech platform addressing a $35B TAM in finance automation (AP automation, treasury ops, spend analytics, invoice fraud controls) growing at 14% CAGR.\n","\n","- **Financial Performance:** Strong organic growth and margin expansion:\n","  - FY2024 Revenue: $495M (+17.9% YoY from $420M in FY2023)\n","  - FY2025E Revenue: $585M (+18.2% YoY)\n","  - Gross Margin: 72–74% (FY2023–2025E), demonstrating software-grade unit economics\n","  - EBITDA Margin: 24–26% (FY2023–2025E); FCF Margin: 18–20%\n","  - Net Debt: $120M\n","\n","- **Valuation Context:** Trading comps (ApexSoft, NorthCloud, PrismData) trade at 5.4–7.1x EV/Revenue and 19.8–25.2x EV/EBITDA. Recent M&A comps (SignalWorks, LedgerIQ) valued at 6.7–8.0x EV/Revenue, reflecting AI-enabled workflow consolidation and vertical expansion premiums.\n","\n","- **Strategic Rationale for Buyers:**\n","  - AI-assisted workflow automation and regulatory report\n"]}]},{"cell_type":"markdown","source":["##9.ARTIFACT GENERATION"],"metadata":{"id":"7JxS9vVQi4yB"}},{"cell_type":"markdown","source":["###9.1.0VERVIEW"],"metadata":{"id":"Wurgn_8fi5-m"}},{"cell_type":"markdown","source":["**Cell 9 — Artifact export: making the run auditable and reviewable**\n","\n","Cell 9 is where we convert a notebook run into professional evidence. In banking, an output that cannot be traced is a liability. A production-grade mindset requires that every run produces artifacts that answer: what happened, with what configuration, in what environment, and what was the final outcome?\n","\n","This cell creates three required exports. First is `run_manifest.json`. The manifest includes the run ID, UTC timestamp, objective description, model lock, temperature, token limits, a hash of the configuration, and an environment fingerprint (Python version, platform, library versions). It also records the controls: no fabrication intent, state-driven routing, bounded loops, explicit end node, and the list of artifacts written. This is the minimum “audit header” you want on any AI-assisted workflow.\n","\n","Second is `graph_spec.json`. This is a structural description of the topology: node list, edge list, entry point, end node, loop bounds, and the model lock. We build it deterministically from our known topology rather than relying on fragile internal introspection. The point is to have a stable, human-readable representation of what the workflow is allowed to do. In governance terms, this is the “process specification.”\n","\n","Third is `final_state.json`. This captures the complete state at termination: section outputs, gap lists, assembled pitchbook, steps executed, and termination reason. Because we used synthetic demo data, exporting full content is acceptable in this notebook. In a real bank, we would apply redaction or restricted storage. But the architectural point remains: we export the state so that reviewers can inspect exactly what happened.\n","\n","The cell also prints confirmation: which files were written and a small manifest snippet. This helps reviewers and students confirm that the artifact pipeline worked.\n","\n","For your bosses, Cell 9 is one of the most persuasive parts of the notebook. It demonstrates that the system is not an opaque text generator. It is a process that produces an audit trail. If a question arises later—“Why did we say this?” or “What did the model see?”—we can answer with artifacts rather than speculation.\n","\n","This is why the notebook is a strong starting point. Many AI prototypes stop at “here is the output.” This one stops at “here is the output, and here is the record of how it was produced.” That is the difference between a demo and a governed capability.\n"],"metadata":{"id":"SYflANI7irC5"}},{"cell_type":"markdown","source":["###9.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"WvgKf0NEi8Tj"}},{"cell_type":"code","source":["# CELL 9/10 — Artifact export: run_manifest.json, graph_spec.json, final_state.json (required)\n","\n","def config_hash(cfg: Dict[str, Any]) -> str:\n","    blob = json.dumps(cfg, sort_keys=True).encode(\"utf-8\")\n","    return hashlib.sha256(blob).hexdigest()[:16]\n","\n","def env_fingerprint() -> Dict[str, Any]:\n","    return {\n","        \"python\": platform.python_version(),\n","        \"platform\": platform.platform(),\n","        \"versions\": {\n","            \"langgraph\": _ver(\"langgraph\"),\n","            \"langchain\": _ver(\"langchain\"),\n","            \"langchain-core\": _ver(\"langchain-core\"),\n","            \"anthropic\": _ver(\"anthropic\"),\n","            \"httpx\": _ver(\"httpx\"),\n","            \"httpcore\": _ver(\"httpcore\"),\n","        }\n","    }\n","\n","def build_graph_spec() -> Dict[str, Any]:\n","    # Deterministic spec from our known topology (no introspection hacks)\n","    nodes = [\"HUB_ROUTER\"] + [f\"WRITE_{sec}\" for sec in DEFAULT_SECTIONS] + [\"AGGREGATE\", \"END\"]\n","    edges = []\n","    edges.append({\"from\": \"HUB_ROUTER\", \"type\": \"conditional\", \"to\": [\"AGGREGATE\"] + [f\"WRITE_{sec}\" for sec in DEFAULT_SECTIONS]})\n","    for sec in DEFAULT_SECTIONS:\n","        edges.append({\"from\": f\"WRITE_{sec}\", \"type\": \"conditional\", \"to\": [\"HUB_ROUTER\", \"AGGREGATE\"]})\n","    edges.append({\"from\": \"AGGREGATE\", \"type\": \"direct\", \"to\": [\"END\"]})\n","    return {\n","        \"name\": \"N7_IB_PITCHBOOK_HUB_SPOKE\",\n","        \"topology\": \"hub_and_spoke_constellation\",\n","        \"nodes\": nodes,\n","        \"edges\": edges,\n","        \"entry_point\": \"HUB_ROUTER\",\n","        \"end_node\": \"END\",\n","        \"loop_bound\": {\"max_steps\": int(final_state.get(\"max_steps\", 0)), \"reason\": \"Classroom-safe bounded routing loop\"},\n","        \"model_lock\": CONFIG[\"model\"],\n","    }\n","\n","run_manifest = {\n","    \"run_id\": final_state.get(\"run_id\"),\n","    \"ts_utc\": final_state.get(\"ts_utc\"),\n","    \"objective\": \"IB pitchbook drafting via hub-and-spoke constellation (LangGraph)\",\n","    \"model\": CONFIG[\"model\"],\n","    \"temperature\": CONFIG[\"temperature\"],\n","    \"max_tokens\": CONFIG[\"max_tokens\"],\n","    \"config_hash\": config_hash(CONFIG),\n","    \"env\": env_fingerprint(),\n","    \"controls\": {\n","        \"no_fabrication\": True,\n","        \"state_driven_routing\": True,\n","        \"bounded_loops\": True,\n","        \"explicit_end_node\": True,\n","        \"artifact_exports\": [\"run_manifest.json\", \"graph_spec.json\", \"final_state.json\"],\n","        \"verification_status\": \"Not verified (synthetic demo)\",\n","    },\n","}\n","\n","graph_spec = build_graph_spec()\n","\n","# Redact large content in final_state export? Requirement says final_state.json required.\n","# We export full state but keep it inspectable; this is synthetic data, so OK.\n","with open(\"run_manifest.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(run_manifest, f, indent=2, sort_keys=True)\n","\n","with open(\"graph_spec.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(graph_spec, f, indent=2, sort_keys=True)\n","\n","with open(\"final_state.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(final_state, f, indent=2, sort_keys=True)\n","\n","print(\"WROTE:\", [\"run_manifest.json\", \"graph_spec.json\", \"final_state.json\"])\n","print(\"MANIFEST_SNIPPET:\", json.dumps({k: run_manifest[k] for k in [\"run_id\",\"ts_utc\",\"model\",\"config_hash\"]}, indent=2))\n"],"metadata":{"id":"Cjgofr2XWIz9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771501751856,"user_tz":360,"elapsed":50,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"24cf81fd-01fd-4ad8-8442-4c87d9e16e10"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["WROTE: ['run_manifest.json', 'graph_spec.json', 'final_state.json']\n","MANIFEST_SNIPPET: {\n","  \"run_id\": \"run_db0ac89110\",\n","  \"ts_utc\": \"2026-02-19T11:45:09.606502+00:00\",\n","  \"model\": \"claude-haiku-4-5-20251001\",\n","  \"config_hash\": \"e741086303188ebb\"\n","}\n"]}]},{"cell_type":"markdown","source":["##10.AUDIT BUNDLE"],"metadata":{"id":"ss5L2x1ojA8d"}},{"cell_type":"markdown","source":["###10.1.OVERVIEW"],"metadata":{"id":"sduxTxx7jC0M"}},{"cell_type":"markdown","source":["**Cell 10 — Integrity checks and fast inspection (closing the loop professionally)**\n","\n","Cell 10 is the “closing gate.” After building a governed workflow, we need a final quick test that confirms the run produced the required artifacts and that key invariants are satisfied. This is not an afterthought; it is a basic professional habit. In a production workflow, you always want an automated check that the deliverables exist and that the system did not silently drift.\n","\n","The cell defines a small helper `file_ok(path)` that checks existence and non-zero size. It then asserts that all required artifacts exist: run manifest, graph spec, and final state. If any are missing, the notebook fails loudly with a clear message. This prevents a scenario where someone believes a run was successful but the evidence was never written.\n","\n","Next, the cell reads `graph_spec.json` and checks critical invariants: the model lock in the spec must match the configured model, and the END node must exist. It also verifies that the topology tag matches “hub_and_spoke_constellation.” These checks may seem simple, but they protect against accidental edits that change the system in ways that reviewers do not notice. In a bank, “small silent drift” is a real operational risk.\n","\n","Finally, the cell prints a compact tail excerpt of the assembled pitchbook. Why the tail? Because the end of the assembled pitchbook includes the consolidated gaps, which is one of the most actionable parts for a deal team. This allows a reviewer to quickly see: “What still needs to be sourced? What questions should we assign to analysts?” It also prints termination reason and step count. That reinforces the bounded behavior story: the system stopped for a defined reason.\n","\n","Pedagogically, Cell 10 teaches an essential principle: **a workflow is not complete until it validates its own outputs**. Many notebooks produce outputs and stop. This notebook produces outputs, exports evidence, and then verifies that the evidence exists and matches the intended constraints.\n","\n","For your bosses, Cell 10 is what makes the system feel operationally serious. It demonstrates that we are thinking like practitioners: build the capability, log it, and validate it. That is exactly the posture required to take this from a lab demonstration to a workflow that could be integrated into a production environment with further hardening (security, data provenance, human approvals, and monitoring).\n","\n","In short, Cell 10 is the final quality gate: it confirms artifacts, confirms invariants, and surfaces the most decision-useful snippet for immediate review.\n"],"metadata":{"id":"bqPR0OmsWKLt"}},{"cell_type":"markdown","source":["###10.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"v_giRgoJjGY5"}},{"cell_type":"code","source":["# CELL 10/10 — Minimal inspection helpers + integrity checks (fast, classroom-suitable)\n","\n","def file_ok(path: str) -> bool:\n","    try:\n","        return os.path.exists(path) and os.path.getsize(path) > 0\n","    except Exception:\n","        return False\n","\n","required = [\"run_manifest.json\", \"graph_spec.json\", \"final_state.json\"]\n","checks = {p: file_ok(p) for p in required}\n","assert all(checks.values()), f\"Missing/empty artifacts: {checks}\"\n","\n","with open(\"graph_spec.json\", \"r\", encoding=\"utf-8\") as f:\n","    gs = json.load(f)\n","\n","assert gs.get(\"model_lock\") == CONFIG[\"model\"], \"Model lock mismatch in graph_spec.\"\n","assert \"END\" in gs.get(\"nodes\", []), \"END node missing in graph_spec.\"\n","assert gs.get(\"topology\") == \"hub_and_spoke_constellation\", \"Topology tag mismatch.\"\n","\n","# Show a compact, decision-useful tail excerpt: consolidated gaps + termination\n","assembled = final_state.get(\"assembled_pitchbook\", \"\")\n","tail = assembled[-900:] if len(assembled) > 900 else assembled\n","\n","print(\"ARTIFACT_CHECKS:\", checks)\n","print(\"TERMINATION:\", final_state.get(\"termination_reason\"), \"| STEPS:\", final_state.get(\"steps\"))\n","print(\"\\n--- PITCHBOOK (tail excerpt) ---\\n\")\n","print(tail)\n"],"metadata":{"id":"Lrqr5DdYjK83","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771501753853,"user_tz":360,"elapsed":7,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"65e4e1e7-ec0a-4394-eb55-9024e9347055"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["ARTIFACT_CHECKS: {'run_manifest.json': True, 'graph_spec.json': True, 'final_state.json': True}\n","TERMINATION: COMPLETED | STEPS: 10\n","\n","--- PITCHBOOK (tail excerpt) ---\n","\n","ors\n","   - Suitable for strategic buyers with balance-sheet capacity or PE sponsors\n","   - Enables full liquidity event; simplifies post-close integration\n","\n","2. **Merger (Statutory or Reverse)**\n","   - Acquirer merges with BlueLedger; potential for earnout/holdback provisions\n","   - Allows seller-side tax planning and phased liquidity\n","   - Facilitates retention of key management via equity rollover\n","\n","3. **Dual-Track IPO Readiness**\n","   - Parallel process: M&A process + IPO preparation\n","   - Strengthens negotiating position; validates public-market appetite\n","   - Requires enhanced disclosure, governance, and financial controls\n","\n","**Use of Proceeds (Illustrative)**\n","- Partial liquidity for sponsors (primary objective)\n","- Accelerate R&D investment (product roadmap expansion)\n","- Selective tuck-in\n","\n","------------------------------------------------------------------------\n","\n","CONSOLIDATED GAPS (cross-section)\n","- None\n"]}]},{"cell_type":"markdown","source":["##11.CONCLUSION"],"metadata":{"id":"ZmL0B12KjLXl"}},{"cell_type":"markdown","source":["**Conclusion: Why This Notebook Is a Strong Starting Point — and How to Take It to Production Grade**\n","\n","What you have in this notebook is not “AI writes a pitchbook.” What you have is the first credible step toward **AI as a controlled drafting system** inside an investment banking workflow. That distinction matters, because the committee’s real question is not whether a model can produce text. Any model can. The real question is whether we can use AI in a way that is **repeatable, reviewable, and safe under professional scrutiny**. This notebook is a strong starting point precisely because it prioritizes structure and control over cleverness.\n","\n","**Why this is a strong starting point**\n","\n","**1) The workflow is explicit, not magical.**  \n","The hub-and-spoke graph makes the process explainable. We can point to the diagram and say: “This is what happens, in this order, with these stop conditions.” That is already far ahead of the typical approach where an analyst pastes prompts into a chat window and hopes the result is consistent. Here, the topology is the policy. It tells us what the system can do and cannot do.\n","\n","**2) The system is modular, which is essential for banking.**  \n","Pitchbooks are not one document; they are a set of standard workstreams. The section-writer spokes map cleanly to real team roles: comps, market, thesis, risks, financial summary, valuation logic, deal structure. Because each section is its own node, we can improve one spoke without destabilizing the others. That is exactly how production systems are built: incremental upgrades, isolated failure domains, and clear responsibilities.\n","\n","**3) The system is state-driven and deterministic in its control flow.**  \n","Routing depends on explicit variables (pending sections, current section, max steps). We are not relying on the model to “decide what to do next” based on conversational text. This is a critical governance property. It means we can reason about failure modes and constrain behavior. Deterministic orchestration is what separates a professional tool from a demo.\n","\n","**4) The notebook is bounded. It cannot run away.**  \n","Bounded loops and explicit termination are not academic details. They are operational safety controls. In a production environment, compute, latency, and unpredictability are risks. This notebook demonstrates that we can cap steps, cap retries, and always reach an explicit END state with a clear termination reason. That predictability is what lets a workflow be embedded into a broader system.\n","\n","**5) It exports an audit trail, not just content.**  \n","The run manifest, graph specification, and final state provide the baseline ingredients of auditability. If a committee asks, “What model did we use? What were the parameters? What did the system see as inputs? What did it produce? Why did it stop?”—we can answer. This is foundational. Without artifacts, the system is not governable.\n","\n","**6) It enforces a “no fabrication” discipline via input packs and gap lists.**  \n","In banking, the most dangerous failure mode is confident invention. This notebook makes a correct philosophical choice: it treats AI as a drafter constrained to known facts, and it forces missing information into a visible **GAPS** section. That turns uncertainty into an explicit to-do list rather than a hidden landmine. In practice, that gap list is one of the highest-value outputs because it guides the next analyst work and prevents the team from mistaking narrative fluency for truth.\n","\n","Taken together, these properties mean you are not just showing “AI content generation.” You are showing a **governed drafting pipeline** whose behavior can be inspected and improved.\n","\n","---\n","\n","**How we take this from “strong starting point” to production grade**\n","\n","To move from a controlled demo to a production system used on live deals, the improvements should follow a bank-like maturity path: **data integrity → governance controls → integration → performance → monitoring**. The good news is the architecture already supports these upgrades. We are not rebuilding; we are extending.\n","\n","**1) Replace synthetic inputs with a controlled data ingestion layer**  \n","Right now, the system uses a synthetic input pack for reproducibility. Production requires a pipeline that builds the input pack from approved sources, with provenance.\n","\n","Concrete upgrades:\n","- **Source connectors**: filings, internal CRM, research PDFs, CapIQ/FactSet exports, internal comps databases, valuation model outputs.\n","- **Normalization**: convert all sources into a standard schema (the “input pack contract”).\n","- **Provenance tags**: every number and statement in the pack should carry source metadata (document, page/section, timestamp, owner).\n","- **Validation checks**: schema validation, missing-field checks, and cross-field consistency (e.g., revenue growth aligns with revenue series).\n","\n","This step matters because in production, the model should not be asked to “find facts.” It should be asked to draft from a **trusted pack** that is already governed.\n","\n","**2) Add a formal verification and consistency layer across sections**  \n","Pitchbooks fail in production mainly through inconsistency: numbers drift, terminology changes, ranges don’t reconcile. A production-grade system needs explicit cross-section QA.\n","\n","Concrete upgrades:\n","- A **Consistency Checker node** after each section (or after aggregation) that verifies:\n","  - The same revenue numbers are used everywhere.\n","  - Multiples cited match the comps table in the pack.\n","  - The thesis statements do not contradict the risks section.\n","  - Key deal terms are consistent across sections.\n","- A **Fact Table** extracted from outputs (structured JSON) so we can compare claims programmatically.\n","- A **Diff-based rerun strategy**: if only one input field changes, only rerun affected sections.\n","\n","This is where AI becomes genuinely valuable: it can draft quickly, and the system can then enforce consistency like a machine.\n","\n","**3) Introduce production governance gates (human-in-the-loop where required)**  \n","In banking, “production grade” is synonymous with “reviewable.” We should formalize gates aligned to real approval processes.\n","\n","Concrete upgrades:\n","- **Suitability and compliance gates**: ensure outputs include required disclaimers and do not include prohibited statements.\n","- **Human sign-off checkpoints**: for example:\n","  - After executive summary + thesis, require VP approval before continuing.\n","  - Before “final deck export,” require MD approval.\n","- **Escalation logic**: if the model reports high gaps, route to a human review node instead of continuing.\n","\n","In graph terms, this means adding controlled branches: **draft → check → approve → proceed**.\n","\n","**4) Upgrade output structure to slide-ready, not just text-ready**  \n","Today the system outputs section text. Production needs format readiness: consistent slide blocks, titles, bullets, and speaker notes.\n","\n","Concrete upgrades:\n","- Standardize outputs into **structured JSON** per section:\n","  - `slide_title`\n","  - `bullets[]`\n","  - `footnotes[]`\n","  - `required_citations[]`\n","  - `gaps[]`\n","  - `speaker_notes`\n","- A “Renderer” stage that converts structured section outputs into:\n","  - PowerPoint templates (internal tooling) or\n","  - Google Slides via API or\n","  - A downstream system that assembles slides.\n","\n","The key principle: keep the model producing **structure**, and let deterministic code handle final formatting.\n","\n","**5) Add retrieval with guardrails (not free-form browsing)**  \n","If we want the system to incorporate documents (CIM, diligence materials, research notes), retrieval must be governed to prevent contamination and to maintain traceability.\n","\n","Concrete upgrades:\n","- Build a retrieval component that:\n","  - retrieves only from approved document stores\n","  - attaches citations to chunks\n","  - passes only minimal needed excerpts into the model\n","- Require the model to produce claim-to-citation mappings.\n","- Treat retrieval as a tool node with logging and access controls.\n","\n","In later notebooks (N8), retrieval becomes the centerpiece. For production, it must be done with strict provenance.\n","\n","**6) Operational hardening: reliability, latency, and cost controls**  \n","Production use is constrained by SLA-style expectations: analysts cannot wait indefinitely, and cost must be predictable.\n","\n","Concrete upgrades:\n","- Timeouts and retries at the API layer (with clear error states).\n","- Caching of section outputs keyed by input pack hash.\n","- Rate limits and concurrency control (parallelize spokes where safe).\n","- A “degraded mode” fallback: if LLM calls fail, produce a skeleton outline and gaps only.\n","\n","The graph should never fail silently. Production means “always returns something usable,” even if it is partial.\n","\n","**7) Security and confidentiality controls**  \n","Pitchbooks often contain MNPI and client confidential information. Production integration must include data governance.\n","\n","Concrete upgrades:\n","- Restrict what is sent to the model: minimum necessary content.\n","- Redaction policies for sensitive fields.\n","- Tenant separation and access controls.\n","- Secure logging that stores hashes and metadata, not raw confidential text, unless explicitly approved.\n","\n","The architecture is already aligned with this mindset because everything flows through the input pack contract.\n","\n","**8) Monitoring and continuous improvement loop**  \n","Production systems must be measured.\n","\n","Concrete upgrades:\n","- Track quality metrics:\n","  - gap rate per section\n","  - inconsistency rate detected by checkers\n","  - edit distance between draft and final approved version\n","  - time saved per pitchbook\n","- Build a “post-mortem” artifact bundle per deal draft.\n","- Create a library of section templates and best-in-class examples to steer outputs.\n","\n","This creates a feedback cycle where the system becomes more reliable over time without becoming less governed.\n","\n","---\n","\n","**The committee-level message**\n","\n","If you need the simplest possible message to close your presentation, it is this:\n","\n","**This notebook proves we can use AI in investment banking the right way: as a controlled drafting workflow with explicit topology, bounded behavior, and audit artifacts. It is a strong starting point because it prioritizes governance and modularity, which are the two things we must get right before adding more power. From here, production grade is a matter of adding trusted data ingestion, consistency checkers, formal approval gates, secure retrieval, and operational hardening. The core architecture already supports those upgrades.**\n","\n","In other words: this is not a flashy demo. It is a reliable foundation. That is why it is worth investing in.\n"],"metadata":{"id":"tv8ReGkpWL8u"}},{"cell_type":"code","source":[],"metadata":{"id":"NQ3kuacawiZE"},"execution_count":null,"outputs":[]}]}