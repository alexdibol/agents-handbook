{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","toc_visible":true,"authorship_tag":"ABX9TyOGnmGK/ex/4DwMb4CYorjX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#**CHAPTER 8. M&A DUE DILIGENCE**\n","---"],"metadata":{"id":"rgzq7ad4oQsb"}},{"cell_type":"markdown","source":["##REFERENCE"],"metadata":{"id":"nBRtjho5h2Cg"}},{"cell_type":"markdown","source":["https://chatgpt.com/share/6997001e-a02c-8012-ad47-158881bfeb80"],"metadata":{"id":"hA-BWhpc2LPZ"}},{"cell_type":"markdown","source":["##0.CONTEXT"],"metadata":{"id":"qWHgDkx8h5mE"}},{"cell_type":"markdown","source":["**Introduction — Why this notebook exists, and what it proves**\n","\n","If you strip away the buzzwords, an M&A due diligence process is a structured way to answer one question: **“What do we believe we are buying, what could go wrong, and what protections do we have?”** In real deals, that question is answered through a controlled workflow: data rooms, legal drafts, finance schedules, management calls, specialist memos, and a constant loop of “find evidence → interpret → identify gaps → escalate.” The practical difficulty is not that we lack documents. The practical difficulty is that we have too many documents, the documents are inconsistent, and time is limited. A committee does not want a long narrative; it wants **traceable conclusions**: what we know, where it came from, and what remains uncertain. That is exactly the behavior this notebook is designed to demonstrate: **AI can assist diligence when it is forced to behave like a governed workflow, not like a chatbot.**\n","\n","This notebook is part of a broader program called **Agentic Architectures in Finance**. The point is not to “make an AI sound smart.” The point is to teach a reliable pattern: **state-driven, auditable decision systems** built with LangGraph. In this notebook (Notebook 8 / N8), the finance use case is **M&A diligence Q&A over documents**, and the architectural dimension we add is: **router + retrieval**. In simple terms, we create an AI workflow that (1) reads a question, (2) decides which diligence domain it belongs to, (3) retrieves the most relevant evidence from the document set, (4) produces an answer strictly grounded in that evidence, and (5) runs a bounded check for missing evidence and escalates gaps. That is the core structure of real diligence. The notebook is a deliberately clean prototype of that structure, designed to be fast enough for a classroom and auditable enough for a professional setting.\n","\n","**What this notebook is (and is not)**\n","\n","This notebook is **not** a replacement for diligence teams, specialist advisors, or investment committee judgment. It does not “verify” truth. It does not discover facts from the world. It does not run a full data room ingestion pipeline. Instead, it demonstrates a smaller, controlled objective:\n","\n","- **Given a defined set of diligence documents** (here, a synthetic mini data room),\n","- **Given a diligence question** (for example, “What buyer protections exist in the SPA?”),\n","- The system can produce a **structured diligence response** that is:\n","  - **Evidence-grounded**: it cites the exact document chunks used.\n","  - **Domain-aware**: it routes the question to the relevant document categories.\n","  - **Gap-aware**: it explicitly lists open items when evidence is missing.\n","  - **Bounded**: it retries evidence collection at most a fixed number of times.\n","  - **Auditable**: it exports run artifacts and a graph topology.\n","\n","Those five properties matter more than “good writing.” They are the difference between a chatbot and a workflow that can be discussed in front of a committee.\n","\n","**The business problem: diligence is a search-and-control problem**\n","\n","In real M&A work, diligence is not a single moment of analysis. It is a loop that repeats under time pressure:\n","\n","1. **A question arrives** (from the deal lead, the IC, legal counsel, the lender, or the buyer).\n","2. A human decides **which domain owns it**:\n","   - legal (SPA/LOI terms, MAE, indemnities),\n","   - financial (quality of earnings, leverage, working capital),\n","   - tax (audits, NOLs, structuring),\n","   - IP (ownership, licenses, transfer restrictions),\n","   - HR (union risk, change of control, key man),\n","   - commercial (customer concentration, churn, pricing power).\n","3. The analyst finds **where the evidence is** (which documents, which sections).\n","4. The analyst forms a view and writes:\n","   - what the documents say,\n","   - what protections exist,\n","   - what liabilities remain,\n","   - what is missing,\n","   - what must be escalated.\n","5. The analyst repeats because new questions and new gaps appear.\n","\n","Most delays, mistakes, and reputational risks come from failures in steps 2–4: wrong domain ownership, missing documents, untraceable claims, and unflagged gaps. An AI system is valuable only if it reduces those failures. But it can only do so if it is forced into the same discipline: routing, retrieval, evidence-only drafting, and explicit escalation.\n","\n","That is why this notebook is designed the way it is. It is not an “AI writes diligence” demo; it is an “AI is made to follow diligence control logic” demo.\n","\n","**The core idea: turn diligence into a state machine**\n","\n","Committees usually hear about AI as if it were magic: you ask, it answers. That is exactly how errors happen. In this notebook we do something different: we treat the diligence process as a **state machine**. A state machine is a formal way of saying: at any moment, the system has a defined status (the “state”), and it moves to the next step based on rules (the “routing”). The state machine approach is how financial institutions control risk: limits, approvals, gates, escalation paths. We simply apply that idea to AI.\n","\n","In practice, the state contains things like:\n","\n","- the question we are trying to answer,\n","- the domain classification (legal, tax, financial, etc.),\n","- the evidence retrieved (document chunks with metadata),\n","- the answer produced,\n","- the citations used,\n","- the open items that remain,\n","- the iteration count (how many times we tried to fetch more evidence),\n","- the termination reason (why we stopped).\n","\n","This is the key shift: **the AI is not “thinking freely.” It is moving through a governed workflow whose current status is visible at all times.**\n","\n","**Why LangGraph: topology as a governance artifact**\n","\n","LangGraph is used because it forces the workflow to be explicit. In normal code, it is easy to hide logic inside functions and end up with untraceable behavior. In LangGraph, you must declare:\n","\n","- nodes (distinct steps),\n","- edges (allowed transitions),\n","- conditional routing (the if/then decisions),\n","- and an explicit END node.\n","\n","This creates a graph you can show to a committee. The graph is not a marketing picture; it is a **contract**: these are the only steps the system is allowed to take.\n","\n","This notebook includes a mandatory visualization step using a pinned Mermaid renderer so the topology is rendered consistently in Colab. That visualization is not cosmetic. It is the learning artifact and the governance artifact: it shows what the system can do, and what it cannot do.\n","\n","**The notebook’s document set: a miniature data room**\n","\n","To keep this notebook fast and deterministic, we use a small synthetic “data room” of representative diligence documents:\n","\n","- LOI terms (price, exclusivity, conditions, leakage concept),\n","- SPA excerpts (closing conditions, MAE, indemnities, caps/baskets, non-compete, reps),\n","- Financial summary (revenue/EBITDA trend, net debt, working capital, concentration),\n","- Commercial memo (cyclicality, concentration, customer termination rights),\n","- HR snapshot (headcount, key executives, union/CBA renewal),\n","- IP register (patents, critical non-transferable license),\n","- Tax note (ongoing audit, exposure range, NOLs).\n","\n","In the real world, these would be PDFs and spreadsheets; here they are text objects, because the point is not OCR. The point is the workflow: **router → retrieval → grounded answer → gap escalation.** Once that workflow is proven, you can replace the synthetic documents with real ingestion and indexing.\n","\n","**Retrieval in this notebook: simple by design, auditable by design**\n","\n","A critical part of diligence is retrieval: finding the relevant paragraphs and clauses. Many AI systems use vector databases and embeddings. That is valid in production, but it adds infrastructure and reduces transparency for teaching. This notebook uses a deliberately simple retrieval method: deterministic token overlap scoring on pre-chunked documents, with a modest document-type boost (e.g., SPA gets a boost for legal queries).\n","\n","Why this matters for a committee: we can explain it without hand-waving. We can say:\n","\n","- We split documents into chunks.\n","- We score each chunk against the question by overlap of keywords.\n","- We take the top K chunks and pass them to the LLM.\n","- We also keep metadata (doc type, title, chunk ID) for audit.\n","\n","This is not “the best retrieval method.” It is the most explainable method for a controlled notebook demo. And it is already enough to show the larger point: **AI assistance depends on evidence selection.** The exact retrieval technology can evolve later.\n","\n","**The LLM role: structured synthesis, not free-form invention**\n","\n","The model used is fixed by the program: **claude-haiku-4-5-20251001**. The important detail is not the brand; the important detail is how we constrain it. In this notebook we constrain the model in two ways:\n","\n","1. **It does not see the whole world.** It sees only the retrieved evidence chunks and the question.\n","2. **It must return strict JSON** with three keys:\n","   - `answer`: what it can conclude from evidence,\n","   - `citations_used`: which chunk IDs it relied on,\n","   - `open_items`: what it cannot conclude and must escalate.\n","\n","This is essential. In professional diligence, the danger is not that the AI makes small mistakes. The danger is that it produces confident language that sounds plausible. The JSON format is an enforcement mechanism: it forces the model to declare its evidence and its uncertainty in a way we can inspect.\n","\n","In other words, the LLM is being used for what it is good at: **summarizing and structuring information** across multiple evidence blocks, while we use the graph and the state machine to handle governance.\n","\n","**The node-by-node story: what happens when you run this notebook**\n","\n","When you run the notebook, the system follows a sequence of nodes. Each node has a single job. Each job updates the state. This is the flow:\n","\n","**1) Intake node**\n","This node ensures the state has a question. In a real system, the question would come from the deal team or a UI. Here we supply an example question like:\n","\n","“Summarize the key buyer protections and seller liabilities in the SPA, and flag any material diligence gaps we should escalate.”\n","\n","The intake node also initializes loop counters and control flags so execution is deterministic.\n","\n","**2) Router node**\n","This node asks the LLM to classify the question into one of the domains: LEGAL, FINANCIAL, TAX, IP, HR, COMMERCIAL, or GENERAL. The router returns strict JSON. If it fails to parse, the system defaults to GENERAL (safe fallback). Then we set a retrieval restriction based on the domain. For example:\n","\n","- LEGAL → SPA and LOI documents\n","- TAX → tax note and SPA\n","- IP → IP register and SPA\n","- FINANCIAL → financials and LOI\n","\n","The key point: this is the first governance control. It prevents the system from “answering legal questions from random documents.” It is a formal version of what a diligence manager does when they say, “This is a legal point—go to the SPA.”\n","\n","**3) Retrieval node**\n","This node retrieves the top K chunks from the documents based on the question and the domain restriction. It records which chunks were selected. In a real diligence workflow, this is the equivalent of pulling the relevant SPA clauses and financial schedule lines into your working memo.\n","\n","**4) Answer node**\n","This node constructs an evidence pack:\n","\n","- Each chunk is labeled with a chunk ID and document metadata.\n","- The LLM is instructed: “Answer using ONLY these evidence blocks.”\n","- The output must be strict JSON: answer, citations, open items.\n","\n","The answer node then filters citations to ensure they match retrieved chunk IDs. That is another governance control: it prevents the model from citing nonexistent evidence.\n","\n","**5) Gap check node (bounded loop)**\n","This node checks if the answer has:\n","- zero citations (meaning: not grounded), or\n","- open items (meaning: evidence gaps remain).\n","\n","If either is true, the system is allowed to do one more retrieval pass — but in a controlled way. Specifically, it broadens the scope to GENERAL (no restriction) and reruns retrieval and answering. This reflects a real behavior: if your first pass looked only at the SPA and you still have gaps, you broaden to other memos, schedules, or notes.\n","\n","Crucially, the loop is bounded. This is important for both governance and compute. The maximum loop iterations are configured (in this notebook, set to 2). That means the system can never spin indefinitely. It will either stop with “EVIDENCE_OK” or stop with “EVIDENCE_GAPS_REMAIN.” A committee can understand and approve that behavior.\n","\n","**6) END node**\n","The graph terminates explicitly. The final state includes:\n","- the answer,\n","- citations,\n","- open items,\n","- termination reason,\n","- and a trace log of node activity.\n","\n","That trace is your audit trail: what decisions were made, and in what order.\n","\n","**What the committee should take away: AI as an assistant with controls**\n","\n","When presenting this to a committee, the most useful framing is:\n","\n","- We are not automating judgment.\n","- We are automating the mechanical part of diligence: routing, searching, summarizing, and gap listing.\n","- We keep humans in control by making outputs traceable and bounded.\n","\n","Specifically, this notebook demonstrates three “committee-grade” properties:\n","\n","**1) Traceability**\n","Every answer is tied to citations that reference specific document chunks. We can always ask: “Where did that come from?” and get an ID that maps to the underlying text.\n","\n","**2) Explicit uncertainty**\n","Open items are not hidden. The system is required to list them. That is critical in diligence where missing a gap is often worse than being uncertain.\n","\n","**3) Governance by topology**\n","The workflow is a graph. The system cannot jump steps. It must follow the defined topology, and we can review the topology as part of model risk governance.\n","\n","**Why this matters operationally: speed, consistency, and safer coverage**\n","\n","In a live deal, diligence questions come constantly, and the work is fragmented: emails, call notes, document versions, redlines. Analysts spend large amounts of time on two tasks:\n","\n","- finding the relevant information,\n","- producing a consistent summary that leadership can digest.\n","\n","This system targets those two tasks. The benefits, if implemented properly, are practical:\n","\n","- **Speed**: faster first-pass answers to common questions.\n","- **Consistency**: standardized format (answer + citations + open items).\n","- **Coverage**: reduced risk of missing obvious clauses because retrieval is systematic.\n","- **Escalation discipline**: open items become a formal checklist.\n","\n","However, the notebook is also honest about limitations. The AI can only answer from what it sees. If documents are missing, it cannot invent them (and it is instructed not to). If the question is ambiguous, it will reflect that in open items. That is the correct behavior.\n","\n","**How to interpret “open items” in a diligence context**\n","\n","A committee will often ask: “So what do we do with the gaps?” The answer is: open items map directly to real diligence actions:\n","\n","- request missing schedules or exhibits,\n","- ask counsel to confirm a clause interpretation,\n","- ask management for clarification,\n","- run specialist review (tax, IP, labor),\n","- incorporate a protection (special indemnity, covenants, escrow),\n","- adjust valuation or deal terms.\n","\n","In other words, open items are not failure; they are the diligence checklist. The system is doing what a good analyst does: **it tells you what you still don’t know.**\n","\n","**Artifacts and audit bundle: why the notebook exports JSON files**\n","\n","A committee also cares about repeatability. If we cannot reproduce outputs, we cannot govern them. That is why the notebook exports three required artifacts:\n","\n","- `run_manifest.json`: what was run, with config hashes and versions.\n","- `graph_spec.json`: the topology and retrieval design, as a machine-readable spec.\n","- `final_state.json`: the final state, including answer, citations, gaps, and trace.\n","\n","This is the beginning of an audit bundle. In a larger system, you would add the exact evidence snippets and document hashes. But even here, we already have the core: **a run can be inspected, compared, and explained.**\n","\n","The notebook also includes a determinism check: rerun the same question with the same configuration and compare fingerprints (excluding timestamps). That is not a gimmick. It’s a control: it ensures the workflow is stable enough to be reviewed.\n","\n","**Connecting the notebook to real implementation: what changes, what stays the same**\n","\n","In a production diligence environment, several pieces would be upgraded:\n","\n","- Document ingestion (PDF parsing, spreadsheets, OCR if needed).\n","- Retrieval (embeddings, vector search, hybrid retrieval, permissions).\n","- Version control (document versions, redlines, superseded drafts).\n","- Access control (deal room permissions, confidentiality).\n","- Logging (PII redaction, secure storage, retention policies).\n","- Human workflow integration (task creation for open items, counsel handoffs).\n","\n","But the most important part would remain unchanged: the graph topology and the state-driven approach. The architecture is portable:\n","\n","- Router node maps questions to domains.\n","- Retrieval node selects evidence.\n","- Answer node synthesizes with citations.\n","- Gap check node enforces escalation.\n","- Bounded loops prevent runaway behavior.\n","- Artifacts support auditability.\n","\n","So when we say “AI can generate diligence,” what we really mean is: **AI can execute a diligence assistant workflow that resembles how analysts work, while producing outputs that leadership can trust because they are traceable and constrained.**\n","\n","**How to present this in simple terms**\n","\n","If you need to summarize the notebook in one minute for leadership, you can say:\n","\n","**“This notebook shows an AI diligence assistant that behaves like a controlled workflow. It first classifies the question (legal, tax, financial), then pulls the most relevant clauses and sections from the deal documents, then produces an answer that is required to cite exactly which clauses it relied on. If it cannot find enough evidence, it says so explicitly and lists the missing items as an escalation checklist. The entire workflow is a visible graph and produces an audit bundle, so we can review how it reached its conclusions.”**\n","\n","That sentence is accurate and aligned with the notebook.\n","\n","**Why the committee should care: it reduces diligence friction while preserving control**\n","\n","Committees rarely approve technology because it is clever. They approve it because it reduces operational friction without increasing risk. This notebook is a demonstration of that balance:\n","\n","- We get speed and standardization.\n","- We keep evidence traceability.\n","- We keep bounded behavior.\n","- We keep explicit uncertainty.\n","- We keep an audit trail.\n","\n","That is the correct argument for AI in diligence. Not “it’s smart,” but “it’s governed.”\n","\n","**Closing perspective: the real goal is institutionalizing the diligence method**\n","\n","The deeper point of this notebook is that diligence quality is not just about talent; it is about process. Great teams have repeatable habits:\n","\n","- they route questions correctly,\n","- they cite evidence,\n","- they separate knowns from unknowns,\n","- they escalate gaps early,\n","- they document decisions.\n","\n","This notebook codifies those habits into an executable graph. That is why it belongs in a program about agentic architectures in finance. It teaches a pattern that can be extended: add a redline comparison node, add a “risk register” node, add a human approval gate node, add a secure retrieval stack. But the foundation stays the same: **diligence as a governed state machine, with AI as a constrained synthesis engine.**\n","\n","That is what you are showing the committee: not a chatbot, but a controlled diligence workflow that produces an answer, a citation trail, and a gap list — and does so in a way that can be audited, repeated, and improved.\n"],"metadata":{"id":"pF_yDrZ7h6_y"}},{"cell_type":"markdown","source":["##1.LIBRARIES AND ENVIRONMENT"],"metadata":{"id":"yeIrVzqFh7fZ"}},{"cell_type":"markdown","source":["**Cell 1 — Install and initialize the notebook in a Colab-safe way**\n","\n","This first cell exists to solve a very practical problem: if the environment is unstable, the “AI diligence workflow” is not credible. In front of a committee, you want to say: **the notebook runs cleanly, it uses explicit dependencies, and it records what it actually ran**. That is why Cell 1 is not “just imports.” It is the foundation for reproducibility and auditability.\n","\n","The key design choice is how we install libraries. Google Colab comes with many packages already installed, and pinning common networking libraries (like `httpx` or `httpcore`) can cause conflicts. So we follow a Colab-safe strategy: we **only** install the minimum project dependencies we control (`langgraph`, `langchain-core`, `anthropic`) and we do it with `--upgrade` rather than hard-pinning a large dependency tree. This reduces the risk of breaking Colab’s internal environment while still ensuring we have compatible versions for LangGraph and the Anthropic client. This is a governance decision: we prefer **stable execution** over micromanaging every transitive dependency.\n","\n","Next, the cell imports the standard Python modules needed for the whole notebook: JSON handling, hashing, deterministic randomness, timestamps, and typing. These are not cosmetic. We use them later to create the run manifest, hash configuration, and keep the workflow deterministic enough for review. We also import LangGraph objects (`StateGraph`, `END`) because the entire notebook is built as an explicit graph, not ad-hoc function calls.\n","\n","Then the cell establishes three pieces of run identity: a `RUN_ID` (unique UUID), a UTC timestamp using timezone-aware `datetime.now(datetime.timezone.utc)`, and an output folder under `/content`. Together, these create an “audit directory” that contains artifacts at the end. This is exactly what professional teams expect: you should be able to point to a folder and say “this run produced these outputs.”\n","\n","Finally, the cell prints a version dictionary. This is crucial for committee-grade credibility: when something changes, we can see whether it is the model, the library versions, or the environment. The cell therefore sets the tone for the entire notebook: **governance-first, deterministic where possible, and always inspectable.**\n"],"metadata":{"id":"GRJDLfL41wsv"}},{"cell_type":"code","source":["# CELL 1/10 — Install + core imports (Colab-safe: avoid pinning common deps that can conflict)\n","# Goal: prevent collisions with Colab preinstalls (especially httpx/httpcore).\n","# Strategy:\n","#   - Do NOT pin httpx/httpcore.\n","#   - Only ensure the 3 project dependencies exist at compatible versions.\n","#   - Use \"--upgrade --quiet\" without hard pins (Colab-friendly).\n","#   - Print the resolved versions for auditability.\n","\n","!pip -q install --upgrade \"langgraph>=0.2.39\" \"langchain-core>=0.3.40\" \"anthropic>=0.34.0\"\n","\n","import os, sys, json, re, uuid, time, random, hashlib, platform\n","import datetime as _dt\n","from dataclasses import dataclass\n","from typing import TypedDict, Literal, Dict, Any, List, Optional, Callable, Tuple\n","\n","from langgraph.graph import StateGraph, END\n","from google.colab import userdata\n","from IPython.display import HTML, display\n","\n","import importlib.metadata as md\n","\n","random.seed(8)\n","os.environ[\"PYTHONHASHSEED\"] = \"8\"\n","\n","def _ver(pkg: str) -> str:\n","    try:\n","        return md.version(pkg)\n","    except Exception:\n","        return \"missing\"\n","\n","RUN_ID = str(uuid.uuid4())\n","TS_UTC = _dt.datetime.now(_dt.timezone.utc).isoformat()\n","\n","OUT_DIR = \"/content/outputs_notebook_8\"\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","VERSIONS = {\n","    \"python\": sys.version.split()[0],\n","    \"platform\": platform.platform(),\n","    \"langgraph\": _ver(\"langgraph\"),\n","    \"langchain-core\": _ver(\"langchain-core\"),\n","    \"anthropic\": _ver(\"anthropic\"),\n","    # helpful diagnostics (may be preinstalled / transitive)\n","    \"httpx\": _ver(\"httpx\"),\n","    \"httpcore\": _ver(\"httpcore\"),\n","}\n","\n","print(\"RUN_ID:\", RUN_ID)\n","print(\"TS_UTC:\", TS_UTC)\n","print(\"OUT_DIR:\", OUT_DIR)\n","print(\"VERSIONS:\", json.dumps(VERSIONS, indent=2))\n"],"metadata":{"id":"sd0qEMbqWFlk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771502831315,"user_tz":360,"elapsed":5835,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"987cf0e2-bf05-4718-f50c-c6fc3a20ec7f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/500.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m491.5/500.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.5/500.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/456.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.3/456.3 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRUN_ID: 3217f4d1-e460-4852-b885-36860a96e99e\n","TS_UTC: 2026-02-19T12:07:11.289953+00:00\n","OUT_DIR: /content/outputs_notebook_8\n","VERSIONS: {\n","  \"python\": \"3.12.12\",\n","  \"platform\": \"Linux-6.6.105+-x86_64-with-glibc2.35\",\n","  \"langgraph\": \"1.0.8\",\n","  \"langchain-core\": \"1.2.13\",\n","  \"anthropic\": \"0.82.0\",\n","  \"httpx\": \"0.28.1\",\n","  \"httpcore\": \"1.0.9\"\n","}\n"]}]},{"cell_type":"markdown","source":["##2.VISUALIZATION STANDARDS"],"metadata":{"id":"aOzs2vNWiEjE"}},{"cell_type":"markdown","source":["###2.1.OVERVIEW"],"metadata":{"id":"_Z7ptw6JiIcM"}},{"cell_type":"markdown","source":["**Cell 2 — Visualization Standard v1: why we render the graph and why it must be hardened**\n","\n","Cell 2 implements the visualization standard. In this program, the graph is not optional; it is a learning artifact and a governance artifact. The committee is not being asked to trust a black box. The committee is being shown the exact workflow topology: which steps exist, in what order they run, and where conditional decisions occur. That is why we require a hardened Mermaid renderer and a dedicated `display_langgraph_mermaid(graph)` function.\n","\n","The practical reason for “hardened” rendering is that Colab can be unreliable with inline JavaScript, and Mermaid versions can change behavior across releases. A diagram that renders one day and fails the next undermines confidence. So the notebook pins Mermaid to a known version (10.6.1 unless explicitly changed) and uses an ESM import from a CDN, which is currently the most stable way to render Mermaid in Colab. The renderer also uses strict security settings to reduce risk and avoid injection issues. Even though this is a classroom notebook, adopting security discipline is part of the point.\n","\n","The renderer works as follows: it creates a unique HTML container ID, injects Mermaid code into a safe string, loads Mermaid as a module, initializes it with predictable settings, and then renders the SVG into the container. If rendering fails, it prints the error inside the notebook rather than failing silently. That “no silent failures” principle is crucial in professional workflows.\n","\n","The second function, `display_langgraph_mermaid(compiled_graph)`, is the interface we standardize across notebooks. It asks LangGraph for its Mermaid representation (using `compiled_graph.get_graph().draw_mermaid()`), then calls the local renderer. This ensures the diagram matches the actual graph topology that will execute. In other words: **we are not drawing an illustrative diagram; we are rendering the diagram from the real program**.\n","\n","When presenting to a committee, this cell lets you say something simple and reassuring: “Here is the workflow the AI is allowed to follow. It cannot invent steps. It cannot skip steps. This diagram is generated from the compiled graph.” That statement is the core reason this cell exists.\n"],"metadata":{"id":"jAsIm-aiiLkE"}},{"cell_type":"markdown","source":["###2.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"AK_Iz2Q4iL4X"}},{"cell_type":"code","source":["# CELL 2/10 — Visualization Standard v1: hardened Mermaid ESM renderer + display_langgraph_mermaid(graph)\n","\n","MERMAID_VERSION = \"10.6.1\"\n","\n","def _safe_id(prefix: str = \"mmd\") -> str:\n","    return f\"{prefix}-{uuid.uuid4().hex[:10]}\"\n","\n","def render_mermaid_locally(mermaid_code: str, *, height_px: int = 520) -> None:\n","    \"\"\"\n","    Hardened Colab Mermaid renderer (ESM). Deterministic + no external state assumptions.\n","    Pinned Mermaid version (default 10.6.1).\n","    \"\"\"\n","    diagram_id = _safe_id(\"mermaid\")\n","    payload = mermaid_code.replace(\"</script>\", \"<\\\\/script>\")\n","    html = f\"\"\"\n","    <div style=\"border:1px solid rgba(0,0,0,0.15); border-radius:12px; padding:12px; overflow:auto;\">\n","      <div id=\"{diagram_id}\" style=\"min-height:{height_px}px;\"></div>\n","    </div>\n","\n","    <script type=\"module\">\n","      import mermaid from \"https://cdn.jsdelivr.net/npm/mermaid@{MERMAID_VERSION}/dist/mermaid.esm.min.mjs\";\n","      mermaid.initialize({{\n","        startOnLoad: false,\n","        securityLevel: \"strict\",\n","        theme: \"default\",\n","        flowchart: {{ curve: \"basis\" }},\n","      }});\n","      const code = `{payload}`;\n","      const el = document.getElementById(\"{diagram_id}\");\n","      try {{\n","        const {{ svg }} = await mermaid.render(\"{diagram_id}-svg\", code);\n","        el.innerHTML = svg;\n","      }} catch (e) {{\n","        el.innerHTML = \"<pre style='color:#b00020; white-space:pre-wrap;'>\" + String(e) + \"</pre>\";\n","      }}\n","    </script>\n","    \"\"\"\n","    display(HTML(html))\n","\n","def display_langgraph_mermaid(compiled_graph: Any) -> None:\n","    \"\"\"\n","    Required by standard. Extracts Mermaid from LangGraph, renders locally.\n","    \"\"\"\n","    mmd = compiled_graph.get_graph().draw_mermaid()\n","    render_mermaid_locally(mmd)\n","\n","print(\"Mermaid renderer ready. Mermaid pinned:\", MERMAID_VERSION)\n"],"metadata":{"id":"IDDyGwjWiO56","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771502857792,"user_tz":360,"elapsed":93,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"cb78745b-ec81-464c-ca27-8899eee9ca10"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mermaid renderer ready. Mermaid pinned: 10.6.1\n"]}]},{"cell_type":"markdown","source":["##3.SYNTHETIC M&A DILIGENCE CORPUS"],"metadata":{"id":"zn9_gpBPh_yn"}},{"cell_type":"markdown","source":["###3.1.OVERVIEW"],"metadata":{"id":"6Sg0lGxQiBj6"}},{"cell_type":"markdown","source":["**Cell 3 — The synthetic “data room”: documents, chunking, and retrieval that is explainable**\n","\n","Cell 3 creates the miniature diligence environment: a set of synthetic documents that represent the kinds of materials you see in a real M&A data room. The purpose is not to simulate every detail of a deal. The purpose is to provide a controlled dataset so we can demonstrate the workflow logic: routing, retrieval, evidence-grounded answering, and gap identification. This is why we use synthetic text rather than PDFs: we are teaching architecture, not document parsing.\n","\n","The document set is intentionally representative. It includes an LOI (high-level terms and conditions), SPA excerpts (closing conditions, MAE, indemnities), a financial summary (revenue/EBITDA, net debt, working capital, concentration), a commercial memo (market/customer risk), HR snapshot (key man, union/CBA timing), IP register (non-transferable license risk), and tax note (audit exposure, NOL limitations). These are exactly the domains that diligence teams triage in real deals. By keeping them short, the notebook remains fast enough for a classroom and predictable enough for repeated runs.\n","\n","Next, the cell defines a deterministic chunking process. In diligence, the unit of evidence is rarely “the whole document.” It is usually a clause, paragraph, or schedule line. Chunking approximates that. The code splits text into chunks under a max character limit, preferring newline boundaries so chunks align with logical sections. Each chunk is labeled with a stable `chunk_id` like `D2::C1`. This ID becomes the anchor for citations later.\n","\n","Then we define retrieval. In production, retrieval might use embeddings and vector search, but that adds infrastructure and reduces transparency. Here we use a simple token-overlap scoring: we tokenize the question, count overlaps with each chunk’s tokens, normalize by length, and apply a small document-type boost (e.g., SPA slightly boosted for legal relevance). This approach is deliberately simple so you can explain it clearly: “We pull the top K chunks with the highest keyword match.”\n","\n","The key point for the committee is: retrieval is not magic. It is a documented, inspectable mechanism that selects evidence. And because chunk IDs and metadata are preserved, we can later show exactly what text supported the answer. This cell makes that possible.\n"],"metadata":{"id":"B7g7qJgZiEOw"}},{"cell_type":"markdown","source":["###3.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"SqN4lQyuiE7Y"}},{"cell_type":"code","source":["# CELL 3/10 — Synthetic M&A diligence corpus + deterministic chunking + retrieval (no vectors; auditable)\n","\n","@dataclass(frozen=True)\n","class Doc:\n","    doc_id: str\n","    title: str\n","    doc_type: Literal[\"LOI\", \"SPA\", \"Financials\", \"HR\", \"Commercial\", \"IP\", \"Ops\", \"Tax\", \"Regulatory\"]\n","    text: str\n","\n","DOCS: List[Doc] = [\n","    Doc(\n","        doc_id=\"D1\",\n","        title=\"Letter of Intent (LOI) — Key Terms\",\n","        doc_type=\"LOI\",\n","        text=(\n","            \"Transaction: acquisition of 100% equity of TargetCo by BuyerCo.\\n\"\n","            \"Indicative purchase price: $420m enterprise value, subject to net debt and working capital adjustments.\\n\"\n","            \"Exclusivity: 45 days from signature.\\n\"\n","            \"Conditions: satisfactory due diligence, financing, board approvals.\\n\"\n","            \"Leakage: prohibited; permitted leakage limited to ordinary-course salaries and agreed items.\\n\"\n","            \"Governing law: New York.\\n\"\n","        ),\n","    ),\n","    Doc(\n","        doc_id=\"D2\",\n","        title=\"Share Purchase Agreement (SPA) — Draft Excerpts\",\n","        doc_type=\"SPA\",\n","        text=(\n","            \"Closing conditions include: antitrust clearance, no material adverse effect (MAE), accuracy of reps and warranties.\\n\"\n","            \"Indemnities: general cap 10% of equity value; basket $2m tipping; survival 18 months.\\n\"\n","            \"Special indemnity: known tax audit for FY2024, capped at $25m, survival 4 years.\\n\"\n","            \"Non-compete: 24 months, territory: North America.\\n\"\n","            \"Representations include: title to shares, financial statements, compliance, IP ownership, employment matters.\\n\"\n","        ),\n","    ),\n","    Doc(\n","        doc_id=\"D3\",\n","        title=\"Financial Statements Summary — FY2023–FY2025 (unaudited)\",\n","        doc_type=\"Financials\",\n","        text=(\n","            \"Revenue: FY2023 $310m; FY2024 $355m; FY2025 $372m.\\n\"\n","            \"EBITDA: FY2023 $52m; FY2024 $61m; FY2025 $58m.\\n\"\n","            \"Net debt (Dec FY2025): $96m.\\n\"\n","            \"Working capital (Dec FY2025): $44m.\\n\"\n","            \"Customer concentration: top-3 customers represent 41% of FY2025 revenue.\\n\"\n","            \"Margin pressure in FY2025 due to input cost volatility and pricing lags.\\n\"\n","        ),\n","    ),\n","    Doc(\n","        doc_id=\"D4\",\n","        title=\"Commercial Diligence Memo — Market & Customers\",\n","        doc_type=\"Commercial\",\n","        text=(\n","            \"TargetCo operates in specialty industrial components with recurring aftermarket demand.\\n\"\n","            \"Key risks: cyclical OEM demand, customer concentration, and competitive pricing pressure.\\n\"\n","            \"Opportunities: cross-selling into BuyerCo channels; pricing optimization; SKU rationalization.\\n\"\n","            \"Major customers have annual renegotiation clauses; one top customer has termination for convenience with 90 days notice.\\n\"\n","        ),\n","    ),\n","    Doc(\n","        doc_id=\"D5\",\n","        title=\"HR Snapshot — Headcount, Key Man, and Benefits\",\n","        doc_type=\"HR\",\n","        text=(\n","            \"Headcount: 820 total; 120 engineering; 540 manufacturing; 160 sales/admin.\\n\"\n","            \"Key executives: CEO (3-year term), CFO (at-will), CTO (2-year retention plan).\\n\"\n","            \"Change-of-control: executive bonus 1.5x base + target; no broad-based severance.\\n\"\n","            \"Union: one manufacturing site unionized; CBA renewal due in 9 months.\\n\"\n","        ),\n","    ),\n","    Doc(\n","        doc_id=\"D6\",\n","        title=\"IP Register — Patents & Licensing\",\n","        doc_type=\"IP\",\n","        text=(\n","            \"Patents: 14 granted; 3 pending; primary families expire 2032–2038.\\n\"\n","            \"Critical software: CAD automation tool licensed from VendorX; license is non-transferable without consent.\\n\"\n","            \"Open-source usage: limited; requires compliance review for copyleft exposure.\\n\"\n","        ),\n","    ),\n","    Doc(\n","        doc_id=\"D7\",\n","        title=\"Tax Diligence Note — Audits & Structure\",\n","        doc_type=\"Tax\",\n","        text=(\n","            \"Ongoing tax audit: FY2024 transfer pricing documentation under review.\\n\"\n","            \"Potential exposure range (management estimate): $8m–$18m including penalties.\\n\"\n","            \"TargetCo has NOLs of $22m; usage subject to change-of-control limitations.\\n\"\n","        ),\n","    ),\n","]\n","\n","def normalize(text: str) -> str:\n","    return re.sub(r\"\\s+\", \" \", text.strip().lower())\n","\n","def tokenize(text: str) -> List[str]:\n","    return re.findall(r\"[a-z0-9]+\", text.lower())\n","\n","def chunk_doc(doc: Doc, *, max_chars: int = 420) -> List[Dict[str, Any]]:\n","    t = doc.text.strip()\n","    parts = []\n","    start = 0\n","    while start < len(t):\n","        end = min(len(t), start + max_chars)\n","        cut = t.rfind(\"\\n\", start, end)\n","        if cut == -1 or cut <= start + 40:\n","            cut = end\n","        chunk = t[start:cut].strip()\n","        if chunk:\n","            parts.append({\n","                \"doc_id\": doc.doc_id,\n","                \"title\": doc.title,\n","                \"doc_type\": doc.doc_type,\n","                \"chunk\": chunk,\n","                \"chunk_id\": f\"{doc.doc_id}::C{len(parts)+1}\",\n","            })\n","        start = cut\n","    return parts\n","\n","CHUNKS: List[Dict[str, Any]] = []\n","for d in DOCS:\n","    CHUNKS.extend(chunk_doc(d))\n","\n","# Deterministic, explainable scoring: weighted token overlap + doc-type boosts\n","DOC_TYPE_BOOST = {\n","    \"SPA\": 1.20,\n","    \"LOI\": 1.10,\n","    \"Financials\": 1.15,\n","    \"Tax\": 1.12,\n","    \"IP\": 1.08,\n","    \"HR\": 1.05,\n","    \"Commercial\": 1.06,\n","    \"Ops\": 1.00,\n","    \"Regulatory\": 1.00,\n","}\n","\n","def retrieve(query: str, *, top_k: int = 5, restrict_types: Optional[List[str]] = None) -> List[Dict[str, Any]]:\n","    q_tokens = tokenize(query)\n","    q_set = set(q_tokens)\n","    if not q_set:\n","        return []\n","\n","    scored = []\n","    for c in CHUNKS:\n","        if restrict_types and c[\"doc_type\"] not in restrict_types:\n","            continue\n","        c_tokens = tokenize(c[\"chunk\"])\n","        overlap = sum(1 for tok in c_tokens if tok in q_set)\n","        # normalize by length to reduce bias for longer chunks\n","        denom = max(12, len(c_tokens))\n","        base = overlap / denom\n","        boost = DOC_TYPE_BOOST.get(c[\"doc_type\"], 1.0)\n","        score = base * boost\n","        if score > 0:\n","            scored.append((score, c))\n","    scored.sort(key=lambda x: (-x[0], x[1][\"chunk_id\"]))\n","    return [dict(item[1], score=float(item[0])) for item in scored[:top_k]]\n","\n","print(\"Docs:\", len(DOCS), \"Chunks:\", len(CHUNKS))\n","print(\"Sample retrieval:\", [r[\"chunk_id\"] for r in retrieve(\"cap basket survival indemnity\", top_k=3)])\n"],"metadata":{"id":"rGOCADsbjRic","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771502886620,"user_tz":360,"elapsed":23,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"ce87da88-a3d0-43d5-a48e-7d659c95a8f3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Docs: 7 Chunks: 14\n","Sample retrieval: ['D2::C1']\n"]}]},{"cell_type":"markdown","source":["##4.STATE SCHEMA"],"metadata":{"id":"7CdFvriyiHh-"}},{"cell_type":"markdown","source":["###4.1.OVERVIEW"],"metadata":{"id":"r7AlxPj_iI11"}},{"cell_type":"markdown","source":["**Cell 4 — The state schema, the AgentNode abstraction, and the strict model wrapper**\n","\n","Cell 4 establishes the “governed system” foundation: a TypedDict state schema, a reusable node abstraction, and a strict wrapper around the LLM call. This is where the notebook stops being “some Python code” and becomes an auditable workflow.\n","\n","First, the cell defines a `RunConfig`. This is important because every meaningful behavior of the system should be controlled by explicit configuration rather than hidden constants. The config includes the fixed model name (strictly `claude-haiku-4-5-20251001`), token limits, temperature (set to 0.0 for stability), loop bounds, and top-K retrieval. When you present to a committee, you want to be able to point to this config and say: “These are the operational settings. They are logged, hashed, and reproducible.”\n","\n","Second, we define utility functions: stable JSON dumps and SHA256 hashing. These are used later to create run manifests and to fingerprint outputs for determinism checks. In finance workflows, you need traceability not just for outputs but for the conditions that produced them.\n","\n","Third, we implement `_anthropic_client()` using `userdata.get(\"ANTHROPIC_API_KEY\")` (ALL CAPS). This is a governance detail: secrets are not hard-coded, and the notebook fails loudly if the key is missing. The `llm_call()` function then wraps the Anthropic SDK in a minimal, consistent interface that returns plain text. This wrapper also helps prevent “hidden behavior” because every call goes through one controlled function.\n","\n","Fourth, we define the `DiligenceState` TypedDict. This is the heart of “state-driven routing.” The state includes the question, the routed domain, the retrieval restrictions, the retrieved chunks, the drafted answer, citations, open items, loop iteration counts, termination reason, and a trace log. This is what makes the system auditable: at any time you can inspect the state and see what the system knows and why it is doing what it is doing.\n","\n","Finally, we define `AgentNode`. Every node in the graph will be an object with a name and a `__call__` method that takes the state and returns an updated state. This makes nodes modular, testable, and composable, and it enforces clean separation between steps. The committee-level takeaway is simple: **we are not letting the model “run the show.” The graph and the state do.**\n"],"metadata":{"id":"ueNd8fXviKfV"}},{"cell_type":"markdown","source":["###4.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"WYXG5UW3iK2O"}},{"cell_type":"code","source":["# CELL 4/10 — Typed state schema + AgentNode abstraction + strict Claude model wrapper (Anthropic_API_KEY)\n","\n","from anthropic import Anthropic\n","\n","MODEL_NAME = \"claude-haiku-4-5-20251001\"\n","\n","@dataclass(frozen=True)\n","class RunConfig:\n","    model: str = MODEL_NAME\n","    max_tokens: int = 700\n","    temperature: float = 0.0\n","    max_loop_iters: int = 2\n","    top_k_retrieval: int = 6\n","\n","def _sha256(s: str) -> str:\n","    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n","\n","def _json_dumps(obj: Any) -> str:\n","    return json.dumps(obj, ensure_ascii=False, indent=2, sort_keys=True)\n","\n","def _anthropic_client() -> Anthropic:\n","    key = userdata.get(\"ANTHROPIC_API_KEY\")  # ALL CAPS (required)\n","    if not key or not isinstance(key, str):\n","        raise RuntimeError(\"Missing Colab secret: userdata.get('ANTHROPIC_API_KEY') (ALL CAPS).\")\n","    return Anthropic(api_key=key)\n","\n","def llm_call(system: str, user: str, *, cfg: RunConfig) -> str:\n","    client = _anthropic_client()\n","    resp = client.messages.create(\n","        model=cfg.model,\n","        max_tokens=cfg.max_tokens,\n","        temperature=cfg.temperature,\n","        system=system,\n","        messages=[{\"role\": \"user\", \"content\": user}],\n","    )\n","    # Anthropic SDK returns a list of content blocks\n","    parts = []\n","    for block in resp.content:\n","        if getattr(block, \"type\", None) == \"text\":\n","            parts.append(block.text)\n","    return \"\\n\".join(parts).strip()\n","\n","class DiligenceState(TypedDict, total=False):\n","    # Inputs\n","    question: str\n","    # Routing\n","    domain: Literal[\"LEGAL\", \"FINANCIAL\", \"TAX\", \"IP\", \"HR\", \"COMMERCIAL\", \"GENERAL\"]\n","    restrict_types: List[str]\n","    # Retrieval\n","    retrieved: List[Dict[str, Any]]  # chunks with metadata + score\n","    # Drafting\n","    answer: str\n","    citations: List[Dict[str, Any]]  # {chunk_id, doc_id, title, doc_type}\n","    open_items: List[str]\n","    # Control + trace\n","    loop_iter: int\n","    needs_more_evidence: bool\n","    termination_reason: str\n","    trace: List[Dict[str, Any]]\n","\n","class AgentNode:\n","    name: str\n","    def __init__(self, name: str):\n","        self.name = name\n","    def __call__(self, state: DiligenceState) -> DiligenceState:\n","        raise NotImplementedError\n","\n","CFG = RunConfig()\n","print(\"MODEL_NAME (strict):\", CFG.model)\n","print(\"RunConfig:\", CFG)\n"],"metadata":{"id":"zwXf8QYdjSeI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771502916025,"user_tz":360,"elapsed":332,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"8f5c3a7a-b725-4d95-ffe7-85eb87f423b0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["MODEL_NAME (strict): claude-haiku-4-5-20251001\n","RunConfig: RunConfig(model='claude-haiku-4-5-20251001', max_tokens=700, temperature=0.0, max_loop_iters=2, top_k_retrieval=6)\n"]}]},{"cell_type":"markdown","source":["##5.NODES"],"metadata":{"id":"PJydwmXniOfj"}},{"cell_type":"markdown","source":["###5.1.OVERVIEW"],"metadata":{"id":"MeZ0d501iP49"}},{"cell_type":"markdown","source":["**Cell 5 — The core agent steps: Intake, Router, Retrieval, Answer, and GapCheck**\n","\n","Cell 5 is where the diligence workflow becomes concrete. We define five nodes, each with a single responsibility, and each designed to update state deterministically and transparently. The principle is: small nodes, clear jobs, no hidden globals, and trace logging at every step.\n","\n","The Intake node is operational hygiene. It guarantees a question exists and initializes control flags like `loop_iter` and `needs_more_evidence`. In professional workflows, you do not want fragile assumptions like “the user always provided a question.” Intake ensures the workflow can start reliably.\n","\n","The Router node is the first “diligence realism” feature. It asks the model to classify the question into a diligence domain: LEGAL, FINANCIAL, TAX, IP, HR, COMMERCIAL, or GENERAL. The output is required to be strict JSON. If parsing fails, we fall back to GENERAL. That fallback is important: it prevents a brittle router from breaking the whole system, and it is safer than guessing. The router then sets `restrict_types` based on the domain (e.g., LEGAL focuses on SPA/LOI). This mimics how real teams assign ownership and narrow the search space.\n","\n","The Retrieval node uses the deterministic retrieval function from Cell 3. It retrieves top-K chunks, optionally restricted by doc type, and stores them in state. It also records which chunks were selected in the trace. In a diligence setting, this is equivalent to assembling the evidence pack before you write.\n","\n","The Answer node is the key governance control. It constructs an “evidence block” where each chunk is labeled and passed to the model. The model is instructed to answer using ONLY that evidence and return strict JSON with `answer`, `citations_used`, and `open_items`. Then we post-process the result: we filter citations so they can only reference chunk IDs we actually retrieved. This prevents hallucinated citations and forces accountability.\n","\n","The GapCheck node implements the bounded evidence loop. If citations are empty or open items exist, the system may do one more pass with broadened retrieval scope. This mimics real diligence: first search in the primary domain, then broaden if gaps remain. But we keep it bounded: after the maximum iterations, the system stops with a termination reason. This is what makes the workflow safe, predictable, and committee-explainable.\n"],"metadata":{"id":"WY3hc3q2iSKx"}},{"cell_type":"markdown","source":["###5.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"fzeMhbzgiShs"}},{"cell_type":"code","source":["# CELL 5/10 — Nodes: Intake → Router → Retrieval → Answer → GapCheck (bounded loop) → END\n","\n","DOMAIN_TO_TYPES = {\n","    \"LEGAL\": [\"SPA\", \"LOI\", \"Regulatory\"],\n","    \"FINANCIAL\": [\"Financials\", \"LOI\"],\n","    \"TAX\": [\"Tax\", \"SPA\"],\n","    \"IP\": [\"IP\", \"SPA\"],\n","    \"HR\": [\"HR\", \"SPA\"],\n","    \"COMMERCIAL\": [\"Commercial\", \"Financials\"],\n","    \"GENERAL\": [],\n","}\n","\n","def _append_trace(state: DiligenceState, node: str, event: Dict[str, Any]) -> None:\n","    t = state.get(\"trace\", [])\n","    t.append({\"ts_utc\": _dt.datetime.now(_dt.timezone.utc).isoformat(), \"node\": node, **event})\n","    state[\"trace\"] = t\n","\n","class IntakeNode(AgentNode):\n","    def __init__(self):\n","        super().__init__(\"intake\")\n","    def __call__(self, state: DiligenceState) -> DiligenceState:\n","        q = (state.get(\"question\") or \"\").strip()\n","        if not q:\n","            state[\"question\"] = \"What are the key risks and protections in the draft SPA and LOI?\"\n","        state[\"loop_iter\"] = int(state.get(\"loop_iter\", 0))\n","        state[\"needs_more_evidence\"] = False\n","        state[\"termination_reason\"] = \"\"\n","        _append_trace(state, self.name, {\"question\": state[\"question\"]})\n","        return state\n","\n","class RouterNode(AgentNode):\n","    def __init__(self, cfg: RunConfig):\n","        super().__init__(\"router\")\n","        self.cfg = cfg\n","\n","    def __call__(self, state: DiligenceState) -> DiligenceState:\n","        q = state[\"question\"]\n","\n","        system = (\n","            \"You are a diligence router. Classify the question into one domain: \"\n","            \"LEGAL, FINANCIAL, TAX, IP, HR, COMMERCIAL, or GENERAL.\\n\"\n","            \"Return strict JSON with keys: domain, rationale_short (<=20 words).\"\n","        )\n","        user = f\"Question:\\n{q}\\n\\nReturn JSON only.\"\n","        raw = llm_call(system, user, cfg=self.cfg)\n","\n","        domain = \"GENERAL\"\n","        try:\n","            obj = json.loads(raw)\n","            d = str(obj.get(\"domain\", \"\")).strip().upper()\n","            if d in DOMAIN_TO_TYPES:\n","                domain = d\n","        except Exception:\n","            domain = \"GENERAL\"\n","\n","        state[\"domain\"] = domain  # type: ignore\n","        restrict = DOMAIN_TO_TYPES.get(domain, [])\n","        state[\"restrict_types\"] = restrict\n","        _append_trace(state, self.name, {\"raw\": raw[:220], \"domain\": domain, \"restrict_types\": restrict})\n","        return state\n","\n","class RetrievalNode(AgentNode):\n","    def __init__(self, cfg: RunConfig):\n","        super().__init__(\"retrieval\")\n","        self.cfg = cfg\n","\n","    def __call__(self, state: DiligenceState) -> DiligenceState:\n","        q = state[\"question\"]\n","        restrict = state.get(\"restrict_types\") or None\n","        hits = retrieve(q, top_k=self.cfg.top_k_retrieval, restrict_types=restrict)\n","        state[\"retrieved\"] = hits\n","\n","        _append_trace(state, self.name, {\n","            \"num_hits\": len(hits),\n","            \"top_chunks\": [h[\"chunk_id\"] for h in hits[:4]],\n","        })\n","        return state\n","\n","class AnswerNode(AgentNode):\n","    def __init__(self, cfg: RunConfig):\n","        super().__init__(\"answer\")\n","        self.cfg = cfg\n","\n","    def __call__(self, state: DiligenceState) -> DiligenceState:\n","        q = state[\"question\"]\n","        hits = state.get(\"retrieved\", [])\n","        domain = state.get(\"domain\", \"GENERAL\")\n","\n","        evidence_lines = []\n","        citations = []\n","        for h in hits:\n","            evidence_lines.append(\n","                f\"[{h['chunk_id']}] ({h['doc_type']}) {h['title']}: {h['chunk']}\"\n","            )\n","            citations.append({\n","                \"chunk_id\": h[\"chunk_id\"],\n","                \"doc_id\": h[\"doc_id\"],\n","                \"title\": h[\"title\"],\n","                \"doc_type\": h[\"doc_type\"],\n","            })\n","\n","        system = (\n","            \"You are an M&A diligence analyst. Answer using ONLY the provided evidence. \"\n","            \"If evidence is insufficient, list open items explicitly.\\n\"\n","            \"Output must be STRICT JSON with keys:\\n\"\n","            \"answer (string), citations_used (array of chunk_ids), open_items (array of strings).\\n\"\n","            \"No extra keys. No markdown.\"\n","        )\n","        user = (\n","            f\"Domain: {domain}\\n\"\n","            f\"Question: {q}\\n\\n\"\n","            f\"EVIDENCE BLOCKS:\\n\" + \"\\n\\n\".join(evidence_lines) + \"\\n\\n\"\n","            \"Return JSON only.\"\n","        )\n","\n","        raw = llm_call(system, user, cfg=self.cfg)\n","\n","        answer = \"\"\n","        used_ids: List[str] = []\n","        open_items: List[str] = []\n","\n","        try:\n","            obj = json.loads(raw)\n","            answer = str(obj.get(\"answer\", \"\")).strip()\n","            used_ids = list(obj.get(\"citations_used\", [])) if isinstance(obj.get(\"citations_used\", []), list) else []\n","            open_items = list(obj.get(\"open_items\", [])) if isinstance(obj.get(\"open_items\", []), list) else []\n","        except Exception:\n","            answer = \"Unable to produce a structured answer. Escalate to human review.\"\n","            used_ids = []\n","            open_items = [\"Model output was not valid JSON. Re-run or escalate.\"]\n","\n","        # Keep only citations that were retrieved\n","        retrieved_ids = {c[\"chunk_id\"] for c in citations}\n","        used_ids = [cid for cid in used_ids if cid in retrieved_ids]\n","\n","        state[\"answer\"] = answer\n","        state[\"citations\"] = [c for c in citations if c[\"chunk_id\"] in used_ids]\n","        state[\"open_items\"] = [str(x).strip() for x in open_items if str(x).strip()]\n","\n","        _append_trace(state, self.name, {\n","            \"raw\": raw[:220],\n","            \"citations_used\": used_ids,\n","            \"open_items_n\": len(state[\"open_items\"]),\n","        })\n","        return state\n","\n","class GapCheckNode(AgentNode):\n","    def __init__(self, cfg: RunConfig):\n","        super().__init__(\"gap_check\")\n","        self.cfg = cfg\n","\n","    def __call__(self, state: DiligenceState) -> DiligenceState:\n","        \"\"\"\n","        Bounded control loop:\n","          - if open_items exist OR citations are empty => attempt one more retrieval pass (broaden scope)\n","          - else END\n","        \"\"\"\n","        it = int(state.get(\"loop_iter\", 0))\n","        open_items = state.get(\"open_items\", [])\n","        cites = state.get(\"citations\", [])\n","\n","        needs = (len(cites) == 0) or (len(open_items) > 0)\n","        state[\"needs_more_evidence\"] = bool(needs)\n","\n","        # If we need more evidence and have remaining iterations, broaden retrieval scope\n","        if needs and it < (self.cfg.max_loop_iters - 1):\n","            state[\"loop_iter\"] = it + 1\n","            # broaden to GENERAL (no restriction) for second pass\n","            state[\"domain\"] = \"GENERAL\"  # type: ignore\n","            state[\"restrict_types\"] = []\n","            _append_trace(state, self.name, {\n","                \"decision\": \"RETRY_BROADER\",\n","                \"loop_iter\": state[\"loop_iter\"],\n","                \"reason\": {\"citations_n\": len(cites), \"open_items_n\": len(open_items)},\n","            })\n","        else:\n","            state[\"termination_reason\"] = \"EVIDENCE_OK\" if not needs else \"EVIDENCE_GAPS_REMAIN\"\n","            _append_trace(state, self.name, {\n","                \"decision\": \"STOP\",\n","                \"loop_iter\": it,\n","                \"termination_reason\": state[\"termination_reason\"],\n","                \"reason\": {\"citations_n\": len(cites), \"open_items_n\": len(open_items)},\n","            })\n","        return state\n","\n","nodes = {\n","    \"intake\": IntakeNode(),\n","    \"router\": RouterNode(CFG),\n","    \"retrieval\": RetrievalNode(CFG),\n","    \"answer\": AnswerNode(CFG),\n","    \"gap_check\": GapCheckNode(CFG),\n","}\n","print(\"Nodes ready:\", list(nodes.keys()))\n"],"metadata":{"id":"t3ZYvRswjUKQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771502941516,"user_tz":360,"elapsed":108,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"1cf06f39-02e3-4886-a1e5-c190138ded37"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Nodes ready: ['intake', 'router', 'retrieval', 'answer', 'gap_check']\n"]}]},{"cell_type":"markdown","source":["##6.GRAPH"],"metadata":{"id":"3qMrcRA8iVGP"}},{"cell_type":"markdown","source":["###6.1.OVERVIEW"],"metadata":{"id":"3Ak9KwaQiXOz"}},{"cell_type":"markdown","source":["**Cell 6 — Building the LangGraph topology: conditional routing and bounded loops**\n","\n","Cell 6 is where we convert the concept into an explicit LangGraph. This matters because it removes ambiguity. Instead of “trust me, the code does X,” we show a topology: these nodes exist, these edges exist, and these are the only allowed transitions. That is a governance win and a teaching win.\n","\n","We start by creating a `StateGraph` using the `DiligenceState` schema defined earlier. This binds the graph to a known state shape. Then we add the five nodes: intake, router, retrieval, answer, and gap_check. The entry point is intake. This ensures every run initializes state properly before routing decisions are made.\n","\n","Next, we connect direct edges in the main path: intake → router → retrieval → answer → gap_check. That is the “normal diligence path”: categorize the question, pull evidence, draft an evidence-grounded answer, then check sufficiency.\n","\n","The key design is the conditional edge leaving gap_check. In LangGraph, conditional routing is explicit: we provide a function that inspects the state and returns the next node name (or END). Here, the routing logic is intentionally conservative and deterministic. The gap_check node itself decides whether a retry is warranted and increments `loop_iter` only when it chooses to broaden retrieval scope. The conditional routing function then checks the trace for that decision and routes back to retrieval if and only if the retry decision occurred. Otherwise, it routes to END.\n","\n","This is important for bounded loops. Many “agentic” demos accidentally create loops that can spin unpredictably, or they rely on text heuristics that are hard to test. Here the loop is bounded by config (`max_loop_iters`) and enforced structurally in the state machine. The system cannot “decide” to loop infinitely. It must follow the rule: at most N passes, with a documented reason each time.\n","\n","Once the topology is declared, we compile the graph. Compilation is a useful control point: it freezes the topology into an executable object and allows us to render the exact graph later. This cell gives you the committee-friendly statement: **“The workflow is a declared graph with explicit conditional routing and bounded loops; it is not an ad-hoc script.”**\n"],"metadata":{"id":"Fnhd7Fwaicyv"}},{"cell_type":"markdown","source":["###6.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"0jFxOakbido7"}},{"cell_type":"code","source":["# CELL 6/10 — LangGraph topology: router + retrieval with bounded retry loop + explicit END\n","\n","graph = StateGraph(DiligenceState)\n","\n","graph.add_node(\"intake\", nodes[\"intake\"])\n","graph.add_node(\"router\", nodes[\"router\"])\n","graph.add_node(\"retrieval\", nodes[\"retrieval\"])\n","graph.add_node(\"answer\", nodes[\"answer\"])\n","graph.add_node(\"gap_check\", nodes[\"gap_check\"])\n","\n","graph.set_entry_point(\"intake\")\n","\n","graph.add_edge(\"intake\", \"router\")\n","graph.add_edge(\"router\", \"retrieval\")\n","graph.add_edge(\"retrieval\", \"answer\")\n","graph.add_edge(\"answer\", \"gap_check\")\n","\n","def route_after_gap_check(state: DiligenceState) -> str:\n","    it = int(state.get(\"loop_iter\", 0))\n","    needs = bool(state.get(\"needs_more_evidence\", False))\n","    # Retry path goes back to retrieval (with broadened scope already set in state)\n","    if needs and it < CFG.max_loop_iters:\n","        # If gap_check decided STOP, it won't have incremented loop_iter; needs will remain True.\n","        # We treat \"loop_iter increment\" as signal of retry.\n","        # If loop_iter was incremented, allow another retrieval+answer pass.\n","        # If not incremented, stop.\n","        # This ensures a deterministic bounded loop.\n","        last_trace = state.get(\"trace\", [])[-1] if state.get(\"trace\") else {}\n","        if last_trace.get(\"node\") == \"gap_check\" and last_trace.get(\"decision\") == \"RETRY_BROADER\":\n","            return \"retrieval\"\n","    return END\n","\n","graph.add_conditional_edges(\"gap_check\", route_after_gap_check, {\"retrieval\": \"retrieval\", END: END})\n","\n","compiled = graph.compile()\n","\n","print(\"Compiled LangGraph. Max loop iters:\", CFG.max_loop_iters)\n"],"metadata":{"id":"HR3pyRxjjV3p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771502961117,"user_tz":360,"elapsed":33,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"4d26715f-01d1-4c96-c860-619d1db0ddf5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Compiled LangGraph. Max loop iters: 2\n"]}]},{"cell_type":"markdown","source":["##7.VISUALIZATION"],"metadata":{"id":"hnqxNRdOigzp"}},{"cell_type":"markdown","source":["###7.1.OVERVIEW"],"metadata":{"id":"9N3hP5ksikQZ"}},{"cell_type":"markdown","source":["**Cell 7 — Rendering the workflow: turning topology into a committee-ready diagram**\n","\n","Cell 7 is short in code but central in meaning. It renders the LangGraph topology using the hardened Mermaid renderer created in Cell 2. In governance-first systems, a diagram is not decoration; it is evidence. It is a compact, readable representation of the system’s permissible behavior.\n","\n","The reason we render after compilation is important: we want the diagram to reflect the exact executable graph, not a hand-drawn approximation. By calling `display_langgraph_mermaid(compiled)`, we ask LangGraph for its Mermaid representation and render it locally. This ensures the diagram matches the true topology: nodes, edges, and conditional transitions.\n","\n","For a committee presentation, this diagram is the fastest way to communicate the architecture in simple terms. You can point to the diagram and explain the flow:\n","\n","- Intake: ensure a valid question and initialize state.\n","- Router: classify the question domain.\n","- Retrieval: pull top evidence chunks.\n","- Answer: synthesize using evidence and produce citations + open items.\n","- Gap check: decide whether evidence is sufficient or whether to broaden and retry.\n","- END: terminate with a clear reason.\n","\n","The diagram also makes bounded looping obvious: the only loop is from gap_check back to retrieval, and it is controlled by state and configuration. That is a strong governance signal: the system is not “free running,” it is executing a controlled process.\n","\n","This cell therefore converts a technical implementation into a human artifact. Committees often struggle with AI because they cannot “see” the system. Here, they can. The diagram is concrete. It shows that we have turned diligence into an explicit procedure, with fixed stages and explicit exit conditions. It is also a didactic tool: once you understand this diagram, you understand the entire notebook. That is why visualization is mandatory in this project standard.\n"],"metadata":{"id":"MzaMhrkuimFO"}},{"cell_type":"markdown","source":["###7.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"epotbIpqimYU"}},{"cell_type":"code","source":["# CELL 7/10 — Mandatory visualization: Mermaid diagram must match topology exactly\n","\n","display_langgraph_mermaid(compiled)\n"],"metadata":{"id":"BTVBYbY4jXuo","colab":{"base_uri":"https://localhost:8080/","height":626},"executionInfo":{"status":"ok","timestamp":1771502974727,"user_tz":360,"elapsed":79,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"35102307-67e8-450b-81a3-d5dea7fdd0fe"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div style=\"border:1px solid rgba(0,0,0,0.15); border-radius:12px; padding:12px; overflow:auto;\">\n","      <div id=\"mermaid-d560a18410\" style=\"min-height:520px;\"></div>\n","    </div>\n","\n","    <script type=\"module\">\n","      import mermaid from \"https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs\";\n","      mermaid.initialize({\n","        startOnLoad: false,\n","        securityLevel: \"strict\",\n","        theme: \"default\",\n","        flowchart: { curve: \"basis\" },\n","      });\n","      const code = `---\n","config:\n","  flowchart:\n","    curve: linear\n","---\n","graph TD;\n","\t__start__([<p>__start__</p>]):::first\n","\tintake(intake)\n","\trouter(router)\n","\tretrieval(retrieval)\n","\tanswer(answer)\n","\tgap_check(gap_check)\n","\t__end__([<p>__end__</p>]):::last\n","\t__start__ --> intake;\n","\tanswer --> gap_check;\n","\tgap_check -.-> __end__;\n","\tgap_check -.-> retrieval;\n","\tintake --> router;\n","\tretrieval --> answer;\n","\trouter --> retrieval;\n","\tclassDef default fill:#f2f0ff,line-height:1.2\n","\tclassDef first fill-opacity:0\n","\tclassDef last fill:#bfb6fc\n","`;\n","      const el = document.getElementById(\"mermaid-d560a18410\");\n","      try {\n","        const { svg } = await mermaid.render(\"mermaid-d560a18410-svg\", code);\n","        el.innerHTML = svg;\n","      } catch (e) {\n","        el.innerHTML = \"<pre style='color:#b00020; white-space:pre-wrap;'>\" + String(e) + \"</pre>\";\n","      }\n","    </script>\n","    "]},"metadata":{}}]},{"cell_type":"markdown","source":["##8.EXECUTION"],"metadata":{"id":"BLus531lirIL"}},{"cell_type":"markdown","source":["###8.1.OVERVIEW"],"metadata":{"id":"x0j2JYTnixuf"}},{"cell_type":"markdown","source":["**Cell 8 — Running the diligence query and producing a readable result plus trace**\n","\n","Cell 8 executes the full workflow on an example diligence question and prints results in a way that is meaningful for practitioners. This is where the committee sees the system “do diligence,” but the most important detail is how we package the output: not just a paragraph, but a structured result that mirrors a diligence memo.\n","\n","We start by defining `format_answer(state)`. This function is deliberately simple and deterministic: it prints the final domain, the termination reason, the answer text, a list of citations, and the open items. This is the presentation layer. In real deployment, you might render to a memo template or push to a deal workflow system, but the idea remains: outputs should be readable and structured for review.\n","\n","Next, we define a realistic question. The example asks for buyer protections, seller liabilities, and escalation gaps. That is exactly how diligence questions are phrased in practice: you want protections, exposures, and what still needs work.\n","\n","We then create an initial state with the question, an empty trace, and loop_iter = 0. This is a key discipline: **we do not rely on hidden memory.** Everything the system needs is in the state.\n","\n","When we call `compiled.invoke(initial_state)`, LangGraph executes the nodes in order, updating state at each step. The system routes, retrieves, answers, and gap-checks. If gaps remain and the bounded loop allows it, it broadens scope and retries once. This is the controlled diligence loop you want to demonstrate.\n","\n","Finally, we print the formatted answer and the last few trace rows. The trace is extremely important in a committee context because it shows the system’s decision process without exposing private chain-of-thought. You can see which node ran, what it decided (e.g., domain classification, which chunks were retrieved), and why it stopped. The trace is your operational accountability: if a stakeholder asks “Why did it pick those documents?” you can point to the retrieval trace. If they ask “Why did it broaden scope?” you can point to the gap_check decision.\n","\n","In short, Cell 8 is the demonstration cell: it runs the governed workflow and produces a committee-readable diligence output with evidence and a trace.\n"],"metadata":{"id":"CGJ3TUQwi2JO"}},{"cell_type":"markdown","source":["###8.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"SCv1wHOni2cy"}},{"cell_type":"code","source":["# CELL 8/10 — Execute: diligence Q&A (router → retrieval → answer; optional bounded retry) + readable output\n","\n","def format_answer(state: DiligenceState) -> str:\n","    lines = []\n","    lines.append(f\"DOMAIN: {state.get('domain','GENERAL')}\")\n","    lines.append(f\"TERMINATION: {state.get('termination_reason','')}\")\n","    lines.append(\"\")\n","    lines.append(\"ANSWER:\")\n","    lines.append(state.get(\"answer\",\"\").strip() or \"(empty)\")\n","    lines.append(\"\")\n","    lines.append(\"CITATIONS:\")\n","    for c in state.get(\"citations\", []):\n","        lines.append(f\"- {c['chunk_id']} | {c['doc_type']} | {c['title']}\")\n","    if not state.get(\"citations\"):\n","        lines.append(\"- (none)\")\n","    lines.append(\"\")\n","    lines.append(\"OPEN ITEMS:\")\n","    for oi in state.get(\"open_items\", []):\n","        lines.append(f\"- {oi}\")\n","    if not state.get(\"open_items\"):\n","        lines.append(\"- (none)\")\n","    return \"\\n\".join(lines)\n","\n","# Example diligence questions (you can edit these)\n","QUESTION = (\n","    \"Summarize the key buyer protections and seller liabilities in the SPA, \"\n","    \"and flag any material diligence gaps we should escalate.\"\n",")\n","\n","initial_state: DiligenceState = {\"question\": QUESTION, \"trace\": [], \"loop_iter\": 0}\n","\n","final_state = compiled.invoke(initial_state)\n","\n","print(format_answer(final_state))\n","print(\"\\nTRACE (last 6):\")\n","for row in final_state.get(\"trace\", [])[-6:]:\n","    print(row)\n"],"metadata":{"id":"s8BmlsxYiw1_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771503031114,"user_tz":360,"elapsed":20888,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"9ade80e3-4708-4df3-bc30-7f719c41e5ba"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["DOMAIN: GENERAL\n","TERMINATION: EVIDENCE_GAPS_REMAIN\n","\n","ANSWER:\n","Unable to produce a structured answer. Escalate to human review.\n","\n","CITATIONS:\n","- (none)\n","\n","OPEN ITEMS:\n","- Model output was not valid JSON. Re-run or escalate.\n","\n","TRACE (last 6):\n","{'ts_utc': '2026-02-19T12:10:13.982607+00:00', 'node': 'retrieval', 'num_hits': 6, 'top_chunks': ['D3::C2', 'D4::C1', 'D5::C2', 'D1::C1']}\n","{'ts_utc': '2026-02-19T12:10:21.691984+00:00', 'node': 'answer', 'raw': '```json\\n{\\n  \"answer\": \"KEY BUYER PROTECTIONS: (1) General indemnity cap of 10% of equity value ($42m) with $2m basket/tipping threshold and 18-month survival period provides limited recourse for breaches. (2) Special ind', 'citations_used': [], 'open_items_n': 1}\n","{'ts_utc': '2026-02-19T12:10:21.692515+00:00', 'node': 'gap_check', 'decision': 'RETRY_BROADER', 'loop_iter': 1, 'reason': {'citations_n': 0, 'open_items_n': 1}}\n","{'ts_utc': '2026-02-19T12:10:21.693149+00:00', 'node': 'retrieval', 'num_hits': 6, 'top_chunks': ['D3::C2', 'D4::C1', 'D5::C2', 'D1::C1']}\n","{'ts_utc': '2026-02-19T12:10:31.065536+00:00', 'node': 'answer', 'raw': '```json\\n{\\n  \"answer\": \"KEY BUYER PROTECTIONS: (1) General indemnity cap of 10% of equity value ($42m) with $2m basket/tipping threshold and 18-month survival period provides limited recourse for breaches. (2) Special ind', 'citations_used': [], 'open_items_n': 1}\n","{'ts_utc': '2026-02-19T12:10:31.066060+00:00', 'node': 'gap_check', 'decision': 'STOP', 'loop_iter': 1, 'termination_reason': 'EVIDENCE_GAPS_REMAIN', 'reason': {'citations_n': 0, 'open_items_n': 1}}\n"]}]},{"cell_type":"markdown","source":["##9.ARTIFACTS"],"metadata":{"id":"7JxS9vVQi4yB"}},{"cell_type":"markdown","source":["###9.1.0VERVIEW"],"metadata":{"id":"Wurgn_8fi5-m"}},{"cell_type":"markdown","source":["**Cell 9 — Exporting required artifacts: run manifest, graph spec, and final state**\n","\n","Cell 9 is where we enforce professional discipline: every run must produce artifacts that can be inspected later. This is not an academic nicety. In finance, if you cannot reproduce and audit a result, you cannot rely on it in decision-making. This cell therefore exports the three required JSON files: `run_manifest.json`, `graph_spec.json`, and `final_state.json`.\n","\n","The `graph_spec.json` is a machine-readable description of the workflow topology. It lists nodes, edges, loop bounds, retrieval method, document types, and the visualization setup. Even if someone never opens the notebook, they can read this JSON and understand what the system is. Importantly, it is not a narrative; it is a specification. That makes it usable for governance reviews and for comparison across notebook versions.\n","\n","The `run_manifest.json` is the run-level audit record. It includes the run ID, timestamp, objective, configuration, configuration hash, question hash, library versions, controls, and a summary outcome (termination reason, iterations, citation count, open item count). Hashes matter because they let you prove that a given output corresponds to a specific configuration. If someone changes the top-K retrieval or loop bounds and reruns, the manifest will reflect that change and the hash will differ.\n","\n","The `final_state.json` is the full end-of-run state. This is the richest artifact: it contains the question, the domain, retrieved evidence metadata, the answer, citations, open items, termination reason, and the trace. This is the closest analogue to a diligence workpaper: it contains both conclusions and the structured record of how those conclusions were produced.\n","\n","A subtle but important point: we write artifacts to a dedicated output directory created in Cell 1. This prevents clutter and makes it easy to package or archive. In a real setting, you would send these outputs to a controlled storage location with access controls and retention rules, but the concept is the same.\n","\n","When presenting to a committee, Cell 9 supports a simple message: **“This system does not just produce text. It produces auditable artifacts that document what happened, how it happened, and what configuration produced it.”**\n"],"metadata":{"id":"SYflANI7irC5"}},{"cell_type":"markdown","source":["###9.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"WvgKf0NEi8Tj"}},{"cell_type":"code","source":["# CELL 9/10 — Required artifacts: run_manifest.json, graph_spec.json, final_state.json (auditable, deterministic)\n","\n","def build_graph_spec() -> Dict[str, Any]:\n","    # Minimal, topology-faithful spec\n","    edges = [\n","        {\"from\": \"intake\", \"to\": \"router\", \"type\": \"direct\"},\n","        {\"from\": \"router\", \"to\": \"retrieval\", \"type\": \"direct\"},\n","        {\"from\": \"retrieval\", \"to\": \"answer\", \"type\": \"direct\"},\n","        {\"from\": \"answer\", \"to\": \"gap_check\", \"type\": \"direct\"},\n","        {\"from\": \"gap_check\", \"to\": \"retrieval\", \"type\": \"conditional\", \"label\": \"RETRY_BROADER\"},\n","        {\"from\": \"gap_check\", \"to\": \"END\", \"type\": \"conditional\", \"label\": \"STOP\"},\n","    ]\n","    return {\n","        \"notebook\": \"AA-FIN-LG-2026 — N8 M&A diligence Q&A (router + retrieval)\",\n","        \"run_id\": RUN_ID,\n","        \"ts_utc\": TS_UTC,\n","        \"model\": CFG.model,\n","        \"nodes\": list(nodes.keys()) + [\"END\"],\n","        \"edges\": edges,\n","        \"loop_bound\": CFG.max_loop_iters,\n","        \"retrieval\": {\n","            \"method\": \"token-overlap\",\n","            \"top_k\": CFG.top_k_retrieval,\n","            \"doc_types\": sorted({d.doc_type for d in DOCS}),\n","            \"num_docs\": len(DOCS),\n","            \"num_chunks\": len(CHUNKS),\n","        },\n","        \"visualization\": {\"mermaid_version\": MERMAID_VERSION, \"renderer\": \"colab_esm_local\"},\n","    }\n","\n","def build_run_manifest(final_state: DiligenceState) -> Dict[str, Any]:\n","    cfg_obj = {\n","        \"model\": CFG.model,\n","        \"max_tokens\": CFG.max_tokens,\n","        \"temperature\": CFG.temperature,\n","        \"max_loop_iters\": CFG.max_loop_iters,\n","        \"top_k_retrieval\": CFG.top_k_retrieval,\n","    }\n","    cfg_hash = _sha256(_json_dumps(cfg_obj))\n","    q_hash = _sha256((final_state.get(\"question\",\"\") or \"\").strip())\n","    return {\n","        \"run_id\": RUN_ID,\n","        \"ts_utc\": TS_UTC,\n","        \"objective\": \"M&A diligence Q&A over documents with router + retrieval and bounded evidence loop\",\n","        \"config\": cfg_obj,\n","        \"config_sha256\": cfg_hash,\n","        \"question_sha256\": q_hash,\n","        \"versions\": VERSIONS,\n","        \"artifacts\": {\n","            \"run_manifest_json\": \"run_manifest.json\",\n","            \"graph_spec_json\": \"graph_spec.json\",\n","            \"final_state_json\": \"final_state.json\",\n","        },\n","        \"controls\": {\n","            \"deterministic_seed\": 8,\n","            \"state_driven_routing\": True,\n","            \"bounded_loop\": True,\n","            \"explicit_end_node\": True,\n","            \"evidence_only_policy\": True,\n","            \"no_hidden_memory\": True,\n","        },\n","        \"outcome\": {\n","            \"termination_reason\": final_state.get(\"termination_reason\",\"\"),\n","            \"loop_iters_executed\": int(final_state.get(\"loop_iter\", 0)) + 1,\n","            \"citations_n\": len(final_state.get(\"citations\", [])),\n","            \"open_items_n\": len(final_state.get(\"open_items\", [])),\n","        },\n","    }\n","\n","graph_spec = build_graph_spec()\n","run_manifest = build_run_manifest(final_state)\n","\n","final_state_path = os.path.join(OUT_DIR, \"final_state.json\")\n","graph_spec_path = os.path.join(OUT_DIR, \"graph_spec.json\")\n","run_manifest_path = os.path.join(OUT_DIR, \"run_manifest.json\")\n","\n","with open(final_state_path, \"w\", encoding=\"utf-8\") as f:\n","    f.write(_json_dumps(final_state))\n","\n","with open(graph_spec_path, \"w\", encoding=\"utf-8\") as f:\n","    f.write(_json_dumps(graph_spec))\n","\n","with open(run_manifest_path, \"w\", encoding=\"utf-8\") as f:\n","    f.write(_json_dumps(run_manifest))\n","\n","print(\"Wrote artifacts:\")\n","print(\"-\", run_manifest_path)\n","print(\"-\", graph_spec_path)\n","print(\"-\", final_state_path)\n"],"metadata":{"id":"Cjgofr2XWIz9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771503054021,"user_tz":360,"elapsed":42,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"c2b5b0d2-a10c-431d-9dfa-51467a1f49e3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote artifacts:\n","- /content/outputs_notebook_8/run_manifest.json\n","- /content/outputs_notebook_8/graph_spec.json\n","- /content/outputs_notebook_8/final_state.json\n"]}]},{"cell_type":"markdown","source":["##10.AUDITR BUNDLE"],"metadata":{"id":"ss5L2x1ojA8d"}},{"cell_type":"markdown","source":["###10.1.OVERVIEW"],"metadata":{"id":"sduxTxx7jC0M"}},{"cell_type":"markdown","source":["**Cell 10 — Determinism and stability check: why it matters and what it tells us**\n","\n","Cell 10 performs a determinism check by rerunning the workflow with the same question and configuration and comparing stable fingerprints of the final states. This is not about claiming perfect determinism in all conditions. It is about demonstrating that the workflow is stable enough to be reviewed and that changes are detectable.\n","\n","In practice, language models can introduce variability. We reduce that variability by setting temperature to 0.0 and by using a bounded, state-driven graph. But we still need a control that tells us: “Did the system behave consistently?” The fingerprinting function provides that control. It hashes the final state after removing timestamps from the trace (timestamps would naturally differ). By excluding timestamps, we focus the check on decisions and outputs: domain classification, retrieved chunks, citations, open items, and answer structure.\n","\n","If the fingerprints match, we have evidence that the workflow is behaving consistently under repeated runs in the same environment. That is valuable for a committee because it increases confidence in the system as a tool for standardized first-pass diligence. If the fingerprints do not match, that is also useful: it tells us where we need stronger controls (for example, stricter router parsing, deterministic retrieval ordering, or tighter answer constraints).\n","\n","This cell also prints artifact file sizes. That is a simple sanity check: the expected files exist and are non-empty. In production, you might validate schemas, sign artifacts, or store them in an immutable bucket, but in a notebook demo the size check is a lightweight confirmation.\n","\n","Conceptually, Cell 10 reinforces the governance-first mindset: we do not just “run AI and hope.” We test the behavior, we measure consistency, and we create signals that can be monitored. Over time, this becomes part of model risk management: you can track drift in outputs, compare runs across versions, and detect changes in routing or retrieval behavior.\n","\n","For your committee, Cell 10 gives you a concrete statement: **“We built this as a controlled system, and we can demonstrate stability and reproducibility characteristics—at least within this bounded notebook setting.”**\n"],"metadata":{"id":"bqPR0OmsWKLt"}},{"cell_type":"markdown","source":["###10.2.CODE AND IMPLEMENTATION"],"metadata":{"id":"v_giRgoJjGY5"}},{"cell_type":"code","source":["# CELL 10/10 — Determinism check (same input, same config): compare final_state hashes + show artifact sizes\n","\n","def state_fingerprint(s: DiligenceState) -> str:\n","    # Exclude timestamps to make determinism test meaningful\n","    s2 = dict(s)\n","    trace = []\n","    for row in s2.get(\"trace\", []):\n","        row2 = dict(row)\n","        row2.pop(\"ts_utc\", None)\n","        trace.append(row2)\n","    s2[\"trace\"] = trace\n","    return _sha256(_json_dumps(s2))\n","\n","# Run again with the same question\n","state2: DiligenceState = {\"question\": QUESTION, \"trace\": [], \"loop_iter\": 0}\n","final_state2 = compiled.invoke(state2)\n","\n","fp1 = state_fingerprint(final_state)\n","fp2 = state_fingerprint(final_state2)\n","\n","print(\"Determinism fingerprints:\")\n","print(\"fp1:\", fp1)\n","print(\"fp2:\", fp2)\n","print(\"MATCH:\", fp1 == fp2)\n","\n","def _filesize(path: str) -> int:\n","    try:\n","        return os.path.getsize(path)\n","    except Exception:\n","        return -1\n","\n","print(\"\\nArtifact sizes (bytes):\")\n","for p in [run_manifest_path, graph_spec_path, final_state_path]:\n","    print(os.path.basename(p), _filesize(p))\n","\n","print(\"\\nDone. Notebook 8 outputs are in:\", OUT_DIR)\n"],"metadata":{"id":"Lrqr5DdYjK83","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771503093471,"user_tz":360,"elapsed":17355,"user":{"displayName":"Alejandro Reynoso del Valle","userId":"04174712603211483042"}},"outputId":"00b0e55f-fcc3-4000-85eb-c70aa6f8159a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Determinism fingerprints:\n","fp1: 80e6a57b726f2918345894b2986e2abae798e49c34010fc3c82ceb70704566bd\n","fp2: 8372c3fd25f225adf3b2de5311164d0c44cd00c6af03918946198ef3b6c2a440\n","MATCH: False\n","\n","Artifact sizes (bytes):\n","run_manifest.json 1272\n","graph_spec.json 1262\n","final_state.json 5384\n","\n","Done. Notebook 8 outputs are in: /content/outputs_notebook_8\n"]}]},{"cell_type":"markdown","source":["##11.CONCLUSION"],"metadata":{"id":"ZmL0B12KjLXl"}},{"cell_type":"markdown","source":["**Conclusion — What we built, what it proves, and how to improve it**\n","\n","This notebook demonstrates a specific, committee-relevant claim: **AI can support M&A due diligence when it is embedded inside a governed workflow.** The model here is not “a chatbot that answers questions.” It is a **state-driven diligence machine** with explicit steps: route the question to a diligence domain, retrieve the most relevant evidence from the document set, draft an answer constrained to that evidence, and then run a bounded gap check that either stops with “evidence OK” or escalates remaining uncertainties as open items. The core output is not just the prose answer; it is the combination of **answer + citations + open items + trace + exported artifacts**. That combination is what makes it usable in a professional environment, because it turns AI output into something that can be reviewed, challenged, and refined like any other diligence work product.\n","\n","The most important contribution of Notebook 8 is the architectural dimension added: **router + retrieval**. This is the bridge between “language model” and “deal work.” In real diligence, analysts do not start by drafting; they start by asking, “Where in the data room does this live?” The router node formalizes that first decision, and the retrieval node formalizes the second decision: “Which specific clauses or sections are relevant?” Only then do we allow synthesis. The result is a model that behaves more like an analyst team: it does not pretend to know everything; it assembles an evidence pack and then produces a structured memo with explicit gaps.\n","\n","There are clear, concrete ways to improve this model while preserving its governance-first design. **First**, upgrade retrieval. The current token-overlap scoring is intentionally simple and auditable, but production diligence benefits from hybrid retrieval: embeddings + keyword filters + metadata constraints (document version, confidentiality tier, deal phase). This can be added without changing the graph topology: the RetrievalNode becomes a pluggable component with a logged scoring explanation. **Second**, add document governance. In real deals, documents have versions, redlines, and superseded drafts. A “Document Registry” node can enforce “latest version” selection, detect conflicts across versions, and attach document hashes for audit. **Third**, add a “Risk Register” node that converts answers and open items into a structured risk log: risk statement, severity, likelihood proxy, owner, mitigation (e.g., escrow, special indemnity, covenant), and linkage to SPA clauses. **Fourth**, add human gates. For committee-grade readiness, introduce an explicit approval node: “Analyst review required” or “Counsel review required” depending on domain and confidence, with a hard END if approval is not granted. **Fifth**, extend the bounded loop from “broaden retrieval” into “targeted follow-ups.” Instead of a generic second pass, the model can generate precise evidence requests: which exhibit, which schedule, which management question—then re-run once those items are provided. That transforms open items into a controlled action workflow.\n","\n","Compared with previous notebooks in this series, the logic and functionality here shift materially. Notebook 1 (personal finance triage) introduced a conditional retry loop for missing information, but the loop was about eliciting inputs. Notebook 2 (suitability boundary) introduced hard branching and early termination—primarily a safety gate for what the system is allowed to do. Notebook 3 (credit memo) introduced a critique loop where a draft is improved through structured feedback. Notebook 4 (trading hypothesis + backtest wrapper) added tool augmentation—using the graph to wrap analysis steps around external computation. Notebook 5 introduced a stateful regime machine for execution tactics, and Notebook 6 added a parallel committee for portfolio decisions, with aggregation. Notebook 7 introduced hub-and-spoke generation for pitchbook sections.\n","\n","Notebook 8 is different because the “hard part” is not generating content; it is **finding and proving.** The new capability is the explicit separation of responsibilities: **routing decides what to look at, retrieval decides what evidence is relevant, and drafting is constrained to what was found.** The loop is not a creative refinement loop; it is an evidence sufficiency loop. This is the first notebook where the system’s primary output is best understood as a diligence artifact: **a traceable answer bound to a data-room evidence pack** plus an escalation list. In that sense, Notebook 8 is the clearest step toward institutional adoption: it mirrors how real diligence is controlled—domain ownership, evidence discipline, and escalation—rather than how a general AI chat behaves.\n","\n","The broader message for the committee is straightforward: this architecture is not trying to replace professional diligence. It is trying to **standardize the first-pass work**—routing, evidence retrieval, structured summarization, and gap surfacing—so senior reviewers spend their time on judgment, negotiation leverage, and risk trade-offs, not on manual searching and reformatting. With improved retrieval, document version governance, risk register generation, and human approval gates, the same topology can scale from a teaching prototype into a controlled diligence assistant that is faster, more consistent, and more auditable than an ungoverned “ask the model” workflow.\n"],"metadata":{"id":"tv8ReGkpWL8u"}}]}